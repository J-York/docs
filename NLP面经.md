# NLP 面经

## NLP 常见任务

文本分类、情感分析、机器翻译、文本摘要、文本生成、问答对话系统、语音识别等

## NLP 常见模型

循环神经网络 RNN，Transformer 模型，RNN 有两种变体 LSTM 和 GRU。

### 循环神经网络

循环神经网络（Recurrent Neural Networks, RNN）是一种用于处理**序列数据**的神经网络，其特点是能够在序列的每个时间步上保持一个隐藏状态，并将其传递给下一个时间步。

<img src="https://img-blog.csdnimg.cn/direct/cad53802a7424deda265df808f0a165a.png" alt="RNN的基本模型结构"  />

<img src="https://img-blog.csdnimg.cn/direct/9049ae5e1469418983d07390de693c68.png" alt=" Pytorch中对RNNCell的定义" style="zoom: 50%;" />

**RNN 的长期依赖问题**：简单来说就是由于 RNN 这种特殊的循环结构，每一步的状态都包含了上一个状态的输出，因此根据链式法则在反向传播求导的时候，越早的项连续相乘的部分就会越长。在处理长序列数据的时候，若连续相乘的项很小就有梯度消失的问题，而相反相乘项比较大的时候就会出现梯度爆炸的问题。

为了解决这个问题，出现了改进版本的RNN，如长短时记忆网络（LSTM）和门控循环单元（GRU），它们通过引入门机制来更好地控制信息的流动，从而增强对长距离依赖的建模能力。

### 长短期神经网络

长短期神经网络是 RNN 的一种，增加了多个**门控结构**，以及新增了一个信息流-------**细胞状态**。

<img src="https://img-blog.csdnimg.cn/direct/7b20aa659e834248ac50313ef59e4b2a.png" alt="LSTM模型结构"  />

x_t 为输入向量

h_t为隐藏状态向量

C_t为细胞状态向量

黄色小框从左到右依次为：f 为**遗忘门**、i **输入门**（中间俩小框）、o **输出门**

- **遗忘门**是决定了从记忆单元中丢弃多少就信息；
- **输入门**也叫**更新门**，是对细胞状态向量的更新；
  - 包含一个 Sigmoid 层决定哪些值将被更新
  - tanh （双曲正切）层生成新的候选值（新信息）
- **输出门**则是由输入、隐藏以及细胞状态向量共同决定模型的输出。

LSTM通过这些特殊的门控结构和细胞状态引入，有效地控制和存储信息，有效地**改善了长期依赖问题**，在一定程度上缓解了梯度爆炸和梯度小时问题。但也因此产生了大量的参数，**训练开销增大**。

### 门控循环单元 （GRU）

门控循环单元是在LSTM（长短期记忆网络）的基础上提出的，**旨在简化LSTM的结构并提高其计算效率**。GRU的设计目标是减少LSTM的参数数量，从而降低计算复杂度，并提高模型的训练速度。

GRU 主要由两个门控机制组成：

- **重置门**（Reset Gate）
  - 决定过去的信息在当前的时间步的隐藏状态中保留多少
- **更新门**（Update Gate）
  - 决定了当前的隐藏状由过去信息和当前信息输入影响多少

<img src="https://img-blog.csdnimg.cn/direct/35fa7aa855a94431af29872514b36dfc.png" alt="GRU模型结构" style="zoom: 25%;" />

### GRU与LSTM的主要区别

1. **结构简单性** ：
   - **门控数量** ：GRU只有两个门（重置门和更新门），而LSTM有三个门（遗忘门、输入门和输出门）。这使得GRU的结构更为简单，参数更少。
   - **记忆单元** ：LSTM具有单独的记忆单元状态，而GRU则直接输出隐藏状态，没有单独的记忆单元。
2. **信息流动** ：
   - **更新方式** ：在LSTM中，输入门和遗忘门共同作用来更新单独的记忆单元，然后通过输出门生成新的隐藏状态。而在GRU中，更新门直接控制信息的保留与更新，减少了信息流动的复杂度。
3. **计算效率** ：
   - GRU因其更简单的结构，通常计算上比LSTM更高效，训练更快，尤其是在小数据集和较小模型中。

### Transformer 模型

它是一种**基于自注意力机制的深度神经网络模型**，特别适用于处理序列数据。

<img src="https://img-blog.csdnimg.cn/direct/f9caf2cc6f8c4c14887509301baf6f05.png" alt="Transformer整体架构" style="zoom: 35%;" />

**Multi-Head Attention：** 这一块是Transformer模型的重点！多头注意力机制是一种扩展自注意力机制的方法，它将自注意力机制分解为多个“头”，每个“头”都在不同的表示空间中学习信息，从而能够捕捉到更丰富的特征和关系。具体的：

- **赋值**： 经过位置编码的矩阵X分别赋值给V、K、Q，因此目前这三个矩阵一模一样且都等于X；
- **线性层Linear：** 矩阵V、K、Q分别输入独立的线性层，进行线性变换，即各自乘上一个参数矩阵。参数矩阵是并行独立训练出来的，所以变换后V、K、Q现在不相同了，但他们维度还是一致的；
- **拆分：** V、K、Q根据头数h进行拆分，多头进行并行工作。在实际代码实现中就是：比如一个3*512的V矩阵，拆分成8头，即变成了8 * 3 * 64的矩阵。这样达到的效果是后续的矩阵运算本质上是每头单独进行运算，而不是一整块进行运算
- **Scaled Dot-Product Attention**: 经过拆分后的V、K、Q输入“缩放点乘积注意力”模块，这个模块是多头注意力机制中的核心模块，基于K、Q矩阵通过一番操作得到输入句子中每个词之间的注意力权重矩阵，然后对值矩阵（V）进行加权求和，从而使得每个词向量都融合上下文的信息，达到了语义增强的目的。该模块数学本质就是下面这个公式。

![image-20240816154212651](C:\Users\j50043562\AppData\Roaming\Typora\typora-user-images\image-20240816154212651.png)

- **掩码操作Mask(opt.)**：Mask包含**Padding Mask**和**Sequence Mask**两种，此处Mask是指**Sequence Mask**。掩码就是让矩阵某些元素变为负无穷的数，使得其在后续Softmax中的概率为0。其中Padding Mask旨在消除输入序列中Padding的影响；模型图中Mask是指Sequence Mask，只存在于解码器中，目的是在预测下一个词时，覆盖住后面的词汇注意力信息，达到只用前面序列来预测下一词的目的。

  <img src="https://img-blog.csdnimg.cn/direct/1a66513a25d647e8897697c9d651883f.png" alt="Multi-Head Attention" style="zoom: 33%;" />

- 相加 & 标准化：将Multi-Head Attention模块输入矩阵和输出矩阵相加，然后将其标准化；
- 前馈神经网络： 将矩阵输入一个两层的全连接网络，激活函数为ReLU。输出维度和输入保持一致；

### BERT 模型

BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer Encoder的预训练语言处理模型

BERT模型的子模块有三个嵌入层

<img src="https://img-blog.csdnimg.cn/direct/f3839947b40b41c887049dc71da1a8bb.png" alt="BERT的输入" style="zoom:50%;" />

- Segment嵌入：仅在处理输入为句子对（NSP）问题时使用。其他任务均赋值为0
- Position嵌入：与Transformer中基于函数的 PostionEncoding 方式不同，BERT 的位置嵌入是基于参数的方式，与词嵌入相似。

BERT 的训练分为两个阶段：预训练阶段与微调阶段

#### 预训练阶段

在预训练阶段，BERT模型使用大量无标注文本进行训练，目的是学习通用的语言表示。

预训练阶段又主要包括两个任务：

1. **Masked Language Model (MLM)** ：
   - **目的** ：通过训练模型去预测句子中被随机掩盖的单词，让模型学习到上下文信息。
   - **方法** ：在输入文本中，随机选择15%的单词，将其中的80%替换成[mask]标记（如"[MASK]"），10%替换成随机单词，10%保持不变。模型的目标是根据上下文预测被掩盖的单词。这种方法使得模型能够建立双向的、上下文敏感的词表示。
2. **Next Sentence Prediction (NSP)** ：
   - **目的** ：帮助模型理解句子间的关系，尤其是在处理问答和自然语言推理任务时。
   - **方法** ：在预训练数据集中，有50%的一对句子为连续句子（即第二句是第一句的下一句），50%为随机选择的句子对。模型需要判别给定的两个句子是否为连贯的上下文。这种任务帮助模型学习文本之间的逻辑关系。

#### 微调阶段

在微调阶段，预训练好的BERT模型会根据具体的下游任务进行调整。

## 描述一下 Transformer 的推理与训练过程

以翻译任务为例。

Transformer模型在处理文本翻译任务时，其推理过程可以分为几个关键步骤。以下是以“你好，世界”为例的详细推理过程：

1. **输入编码**：
   - 首先，模型会将输入文本“你好，世界”转换为token序列。假设tokenizer将这句话分解为“你”、“好”、“，”、“世”、“界”。
   - 每个token会被转换为一个embedding向量，这些向量包含了词汇的语义信息。
2. **位置编码**：
   - 由于Transformer模型本身不包含循环结构，无法自然地处理序列的顺序信息，因此需要添加位置编码。位置编码会与token embedding相加，以提供序列中每个token的位置信息。
3. **自注意力机制**：
   - 输入序列（包含位置编码的token embedding）会被送入多个自注意力层。在自注意力层中，模型会计算每个token与序列中其他token的关系，从而捕捉序列内部的依赖关系。
   - 自注意力机制通过计算query、key、value矩阵来实现这一点，具体是通过点积计算注意力分数，然后对value进行加权求和。
4. **前馈神经网络**：
   - 每个自注意力层的输出会被送入一个前馈神经网络，该网络对每个token进行非线性变换，进一步提取特征。
5. **编码器堆叠**：
   - 上述的自注意力和前馈神经网络步骤会被重复多次（通常是6层或更多），形成编码器堆叠结构。每一层都会对输入序列进行更深层次的特征提取。
6. **解码器操作**：
   - 在翻译任务中，解码器会接收编码器的输出，并生成目标语言的token序列。解码器同样包含自注意力和前馈神经网络层，但在自注意力层中，解码器还会考虑编码器的输出。
   - 解码器在生成每个token时，会使用masked自注意力机制，确保每个token只能依赖于它之前的token，以保持自回归特性。
7. **输出预测**：
   - 解码器生成目标语言的token序列后，模型会通过一个线性层和softmax函数来预测下一个token的概率分布。
   - 在推理过程中，通常会使用贪婪搜索或束搜索等策略来选择最可能的token序列。
8. **输出解码**：
   - 最终，模型会输出翻译结果，例如“Hello, World”。

整个推理过程是端到端的，从输入文本到输出翻译结果，Transformer模型通过其独特的自注意力和编码器-解码器结构，能够有效地捕捉长距离依赖关系，并生成高质量的翻译文本。

## 介绍一下 Embedding 技术

在NLP领域中Embedding即**词嵌入技术**，是一种将文本中的单词转换为固定长度的向量（词向量）技术。

### 为什么需要Embedding技术

1. 机器学习需要数字化表达，因此文本数字化表达为向量才能被模型理解。
2. 维度灾难。独热编码会导致特征向量极度稀疏。
3. 词汇鸿沟。独热编码不能表达词汇之间的联系。

## 在Transformer中计算自注意力机制的时候，为什么除以 sqrt(d_k)，为什么不直接除以d_k，为什么不除以 d_k 的1/3次幂

1. 为什么要进行缩放: 首先，进行缩放的主要目的是为了控制softmax函数的输入值的方差。如果不进行缩放，随着维度d_k的增加，点积的结果会变得很大，导致softmax函数的梯度变得很小，这会影响模型的学习。

2. 为什么选择sqrt(d_k): 选择sqrt(d_k)是基于理论分析和经验的结果。假设查询(Q)和键(K)的各个分量是独立同分布的随机变量，均值为0，方差为1。那么它们的点积的方差将正比于d_k。通过除以sqrt(d_k)，我们可以将点积的方差缩放回1，保持一个稳定的分布。

3. 为什么不直接除以d_k: 如果直接除以d_k，缩放可能会过度，使得点积的值变得过小。这可能会导致梯度消失问题，特别是在d_k较大的情况下。

4. 为什么不除以d_k的1/3次幂: 虽然d_k的1/3次幂也是一种可能的选择，但它可能不如sqrt(d_k)有效。sqrt(d_k)提供了一个很好的平衡，既能控制方差，又不会过度压缩值的范围。

## 在Self Attention中，Q K V 一般会怎么初始化，如果使用全0初始化或者全1初始化可能会有什么问题

1. 常见的初始化方法： a) 随机初始化：通常使用均匀分布或正态分布进行随机初始化。 b) Xavier/Glorot初始化：根据输入和输出的神经元数量来缩放初始化值。 c) He初始化：类似于Xavier初始化，但专门为ReLU激活函数设计。

2. 全0初始化的问题： 如果使用全0初始化Q、K、V矩阵，可能会导致以下问题： a) 对称性问题：所有的注意力权重将会相同，导致模型无法学习到不同位置之间的差异。 b) 梯度消失：在反向传播时，由于所有权重都是0，梯度也会变为0，阻止模型学习。 c) 表达能力受限：模型将无法捕捉到输入序列中的任何模式或特征。

3. 全1初始化的问题： 全1初始化同样会带来一些问题： a) 注意力分数过大：所有的注意力分数都会非常高，导致softmax函数的输出接近均匀分布。 b) 梯度爆炸：在反向传播时可能导致梯度爆炸，特别是在深层网络中。 c) 难以区分重要性：模型难以学习到哪些部分更重要，因为初始状态下所有位置都被同等对待。

4. 为什么需要适当的初始化： 合适的初始化可以： a) 打破对称性，使得不同的神经元可以学习不同的特征。 b) 控制激活值和梯度的方差，防止梯度消失或爆炸。 c) 加速收敛，使模型更容易找到好的局部最优解。

### Xavier 初始化

Xavier初始化的作者，Xavier Glorot，在[Understanding the difficulty of training deep feedforward neural networks](https://cloud.tencent.com/developer/tools/blog-entry?target=https%3A%2F%2Flinks.jianshu.com%2Fgo%3Fto%3Dhttp%3A%2F%2Fproceedings.mlr.press%2Fv9%2Fglorot10a%2Fglorot10a.pdf&source=article&objectId=1587082)论文中提出一个洞见：激活值的方差是逐层递减的，这导致反向传播中的梯度也逐层递减。要解决梯度消失，就要避免激活值方差的衰减，最理想的情况是，每层的输出值（激活值）保持高斯分布。

因此，他提出了Xavier初始化：bias初始化为0，为Normalize后的参数乘以一个rescale系数：1 / \sqrt{n} ，n是输入参数的个数。

### He 初始化

Xavier初始化的问题在于，它只适用于线性激活函数，但实际上，对于深层神经网络来说，线性激活函数是没有价值，神经网络需要非线性激活函数来构建复杂的非线性系统。今天的神经网络普遍使用relu激活函数。

因为relu会抛弃掉小于0的值，对于一个均值为0的data来说，这就相当于砍掉了一半的值，这样一来，均值就会变大，根据新公式的推导，最终得到新的rescale系数：\sqrt{2 / n} 。

## 如何避免模型的过拟合，如果训练数据量足够大，就一定不会过拟合吗

### 1. **增加数据量**

- **获取更多数据**：更多的数据可以帮助模型更好地学习，减少对特定训练数据的依赖。
- **数据增强**：通过随机裁剪、旋转、翻转、添加噪声等方式生成更多的训练样本，特别是在图像和文本处理领域。

### 2. **正则化**

- **L1/L2正则化**：向损失函数中添加权重惩罚项，L1正则化会导致稀疏解，L2正则化则使权重减小，防止模型过于复杂。
- **Dropout**：在每次训练时，随机丢弃一部分神经元，防止神经网络对某些特定路径过度依赖。
- **Early Stopping（提前停止）**：在验证集的损失不再降低时停止训练，避免模型继续过拟合训练数据。

### 3. **简化模型**

- **减少模型复杂度**：通过降低模型的参数数量，减少神经网络的层数或每层的神经元数量来降低模型的复杂度。
- **特征选择**：移除不相关或噪声较大的特征，降低模型的复杂性。

### 4. **交叉验证**

- **K折交叉验证**：将数据集分为K份，轮流将其中一份作为验证集，剩余的作为训练集。通过多次训练与验证，减少模型对特定数据的依赖，提高模型的泛化能力。

### 5. **集成学习**

- **Bagging**：通过训练多个独立的模型并将它们的结果进行平均或投票，可以减少单一模型的过拟合风险。随机森林就是一个常见的例子。
- **Boosting**：通过训练一系列弱学习器，并逐步增强难以预测的样本，提升整体模型的性能，同时也可以减少过拟合。

### 6. **数据预处理**

- **归一化/标准化**：对数据进行归一化或标准化，使特征值在同一尺度上，有助于模型更稳定的收敛。
- **移除噪声**：清理数据中的异常值或噪声，减少不必要的复杂度。

## L1 正则化与 L2 正则化

### L1正则化（Lasso Regularization）

L1正则化是在模型的损失函数中添加权重的绝对值之和作为惩罚项。

**特点** ：

- **稀疏性** ：L1正则化倾向于产生**稀疏的权重矩阵**，即很多权重参数会被强制设置为0。这种特性使得L1正则化可以用作**特征选择**，因为它能够自动地忽略那些不重要的特征。
- **对异常值不敏感** ：由于惩罚的是权重的绝对值，因此L1正则化对于异常值有一定的鲁棒性。

### L2正则化（Ridge Regularization）

L2正则化是在模型的损失函数中添加权重的平方和作为惩罚项。常见的是使用权重向量的L2范数的平方。

**特点** ：

- **权重衰减** ：L2正则化通过惩罚权重的平方，使得权重值趋向于减小，但不会变成0。这有助于防止模型过拟合，因为它减少了模型对单个权重的高度依赖。
- **稳定性** ：相比于L1正则化，L2正则化通常会导致**更稳定的权重更新**，因为它不会导致权重参数的急剧变化。

>  当特征数量很多，且希望自动进行特征选择时，L1正则化是一个好的选择。而当目标是减少过拟合，但不希望减少特征数量时，L2正则化更为合适。

## Dropout 是什么，有什么用

Dropout是一种用于深度学习网络中的正则化技术，旨在防止模型过拟合。它由Hinton等人在2012年提出，并在神经网络训练中得到了广泛应用。

### Dropout的工作原理：

在训练过程中，Dropout会随机“丢弃”（即设置为零）网络中的一部分神经元，包括它的权重和激活。具体来说，对于每个训练批次，每个神经元都有一定的概率 p（称为dropout rate）被暂时从网络中移除。在剩余的神经元上继续进行前向传播和反向传播，但**权重更新时会考虑到被丢弃的神经元**。

### Dropout的作用：

1. **防止过拟合** ：通过随机丢弃神经元，Dropout减少了网络对特定神经元的依赖，迫使网络学习更加鲁棒的特征表示，这有助于提高模型在未见数据上的泛化能力。
2. **实现模型集成** ：Dropout可以看作是一种廉价的模型集成方法。每次迭代时，由于不同的神经元被丢弃，实际上是在训练不同的网络结构。在测试时，所有这些网络的预测可以以某种方式（如取平均值）结合起来，从而提高模型的性能。
3. **减少参数数量** ：虽然Dropout本身并不直接减少模型参数的数量，但由于它在训练过程中减少了活跃神经元的数量，可以在一定程度上减少模型对参数的依赖。

### Dropout的使用：

- **设置dropout rate**：通常，dropout rate设置在0.2到0.5之间，这个值需要根据具体问题和网络结构进行调整。
- **训练与测试** ：在训练时应用dropout，而在测试时则不应用。为了在测试时保持输出的一致性，通常需要在训练时将每个神经元的输出乘以1−p，这样在测试时就不需要任何修改，因为所有神经元的输出都被“保留”。

### Dropout的注意事项：

- **批量大小** ：在使用Dropout时，建议使用较大的批量大小，因为这样可以减少随机性的影响，使梯度估计更加稳定。
- **学习率** ：在使用Dropout时，可能需要调整学习率，因为网络的有效容量会因为Dropout而降低。
- **与其他正则化技术结合** ：Dropout可以与其他正则化技术（如L1、L2正则化）结合使用，以达到更好的正则化效果。

## 常用的优化器有哪些，Adam的有哪些优点

### 1. **梯度下降法（Gradient Descent）**

- **批量梯度下降（Batch Gradient Descent）**
  - **概述**：在每次迭代中，使用整个训练集计算梯度并更新参数。
  - **优点**：收敛到全局最优解，更新方向稳定。
  - **缺点**：对内存要求高，计算开销大，尤其是当训练集很大时。
- **随机梯度下降（Stochastic Gradient Descent, SGD）**
  - **概述**：在每次迭代中，只使用一个样本计算梯度并更新参数。
  - **优点**：计算效率高，对内存要求低，可以跳出局部最优解。
  - **缺点**：更新方向不稳定，容易受到噪声影响，收敛较慢。
- **小批量梯度下降（Mini-batch Gradient Descent）**
  - **概述**：在每次迭代中，使用一个小批量的样本（如32或64个样本）计算梯度并更新参数。
  - **优点**：在计算效率和稳定性之间取得平衡，比SGD收敛更快且更稳定。
  - **缺点**：可能仍会陷入局部最优解。

### 2. **动量法（Momentum，SGDM）**

- **概述**：在每次更新时，结合当前梯度和上一次更新方向的加权平均，类似于物理中的动量概念。
- **优点**：可以加速收敛，尤其是在长而平坦的谷底；有助于跳出局部最优解。
- **缺点**：需要选择合适的动量参数（通常是0.9），否则可能导致振荡。

### 3. **Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）**

- **概述**：在动量法的基础上，首先利用动量估计未来的位置，然后在此位置上计算梯度。
- **优点**：比普通动量法收敛更快，更早地纠正了更新方向，避免过冲。
- **缺点**：增加了计算复杂性。

### 4. **AdaGrad（Adaptive Gradient）**

- **概述**：为每个参数维护一个自适应的学习率，学习率根据参数历史梯度的平方和的倒数进行调整。通过对每个参数的历史梯度平方和进行累加，从而调整每个参数的学习率。频繁更新的参数会得到较小的学习率，而不常更新的参数会得到较大的学习率。
- **优点**：适合处理稀疏数据，能够自动调整学习率。
- **缺点**：学习率随着时间推移不断减小，可能导致过早停止学习。

### 5. **RMSProp（Root Mean Square Propagation，根均方传播）**

- **概述**：对 AdaGrad 进行改进，使用**指数加权移动平均数**来累积平方梯度，防止学习率过早减小。
- **优点**：适用于非平稳目标（如深度学习中的损失函数），收敛更快且更稳定。
- **缺点**：引入了超参数（通常是0.9），需要调优。

### 6. **Adam（Adaptive Moment Estimation，自适应矩估计）**

- **概述**：Adam结合了动量法和 RMSProp 的思想，同时利用梯度的一阶矩（动量）和二阶矩（平方梯度）的指数移动平均值来调整学习率。
  - 在统计学中，一阶矩通常指的是随机变量的期望值，即平均值。在优化算法中，一阶矩指的是梯度的指数移动平均值，它可以看作是梯度的“动量”部分。在Adam算法中，一阶矩用于累积梯度的历史信息，帮助算法在梯度方向一致的维度上加速更新。
  - 二阶矩在统计学中通常指的是随机变量的方差。在优化算法中，二阶矩指的是梯度平方的指数移动平均值，它可以看作是梯度的“均方根”部分。在Adam算法中，二阶矩用于自适应地调整每个参数的学习率，使得在梯度较大的维度上减小学习率，在梯度较小的维度上增大学习率。

- **优点**：广泛应用于深度学习，能够快速收敛，自适应调整学习率，对初始学习率敏感度较低。
- **缺点**：可能需要进行超参数调优（如学习率、动量参数等），在某些情况下可能不如SGD泛化能力好。

## 了解过 Boosting 系列算法吗

Boosting系列的算法是一组集成学习算法，它们通过顺序地训练一系列弱学习器（通常是决策树），并将它们组合成一个强学习器，以此来提高整体的预测性能。Boosting算法的核心思想是利用一系列的弱学习器，通过加权投票的方式，形成一个性能更好的强学习器。

以下是Boosting系列算法的一些关键特点：

1. **顺序性** ：Boosting算法是顺序训练的，每个新的学习器都尝试纠正前一个学习器的错误。
2. **权重调整** ：在训练过程中，每个样本的权重会根据前一个学习器的表现进行调整。被错误分类的样本权重会增加，而被正确分类的样本权重会减少，这样后续的学习器会更加关注难分的样本。
3. **弱学习器** ：Boosting算法通常使用简单的模型作为基学习器，这些模型单独表现一般，但组合起来可以非常强大。
4. **加权组合** ：每个学习器在最终的预测中都有一定的权重，这些权重是基于学习器在训练过程中的表现来确定的。

以下是几种流行的Boosting算法：

- **AdaBoost（Adaptive Boosting）** ：是最著名的Boosting算法之一，它通过调整样本权重来训练一系列的分类器，并将这些分类器加权组合起来。
- **Gradient Boosting**：是一种通用的框架，它可以用于分类和回归问题。它通过最小化损失函数来训练一系列的**决策树**，每棵树都是在前一棵树的基础上构建的。
- **XGBoost**：是一个优化的分布式Gradient Boosting库，它提供了快速的运算和更好的模型性能。XGBoost在系统优化和算法实现上做了很多改进。
- **LightGBM**：是另一个高效的Gradient Boosting框架，它使用了基于直方图的算法和GOSS（Gradient-based One-Side Sampling）等技术来提高训练速度和减少内存使用。
- **CatBoost**：是一个基于对称决策树的Gradient Boosting算法，它**特别擅长处理类别特征**，并且减少了过拟合的风险。

Boosting算法在许多机器学习竞赛和实际应用中表现出色，尤其是在结构化数据上。然而，它们也可能导致过拟合，特别是当模型非常复杂或者训练数据量不足时。因此，在使用Boosting算法时，需要进行适当的正则化，并且仔细调整超参数。

### 区别：

- **样本权重更新** ：AdaBoost通过改变样本权重来影响后续的分类器，而Gradient Boosting、XGBoost和LightGBM则是通过拟合残差来构建模型。
- **损失函数** ：AdaBoost通常用于分类问题，而Gradient Boosting、XGBoost和LightGBM可以用于回归和分类问题，并且可以自定义损失函数。
- **优化和效率** ：XGBoost和LightGBM在系统优化方面做了很多工作，使得它们在处理大规模数据时更加高效。
- **正则化** ：XGBoost和LightGBM在模型训练中加入了更多的正则化项，有助于防止过拟合。

## Llama 中的旋转位置编码和绝对位置编码有什么区别？如果序列很长，用什么编码方式比较适合？

Llama模型中的旋转位置编码（RoPE，Rotation Position Encoding）和绝对位置编码（Absolute Position Encoding）都是用来在Transformer模型中引入位置信息的方法。它们的主要区别在于编码方式和效果上：

1. **旋转位置编码（RoPE）** ：
   - **原理** ：RoPE通过将每个位置的编码与一个旋转矩阵相乘来引入位置信息。这种方法允许模型捕获位置之间的相对关系，因为旋转矩阵的乘法本质上是一种相对位置的操作。
   - **优点** ：可以更好地处理长序列，因为它是相对位置的，不受序列长度的影响。
   - **适用性** ：对于非常长的序列，RoPE通常更为有效，因为它能够保持相对位置信息的准确性。
2. **绝对位置编码** ：
   - **原理** ：绝对位置编码为序列中的每个位置分配一个唯一的编码向量。这些向量通常是固定的或者通过学习得到的，直接加到输入嵌入上。
   - **优点** ：实现简单，对于不太长的序列，能够有效引入位置信息。
   - **缺点** ：当序列长度增加时，绝对位置编码可能无法很好地处理超出训练时最长序列长度的位置信息。

## 什么是奥卡姆剃刀原则

奥卡姆剃刀原则（Occam's Razor）是一个解决问题的原则，它指出在解释任何现象时，应当尽可能少地做出假设。其核心理念是“**如无必要，勿增实体**”，也就是说，在没有充分理由的情况下，不应该引入更多复杂的概念或假设。

简单来说，奥卡姆剃刀原则鼓励我们选择最简单、最少假设的解释作为最佳解释。这个原则并不一定意味着最简单的解释就是正确的，但它提供了一个优先考虑简单性的标准，因为在没有额外证据支持更复杂解释的情况下，简单的解释往往更可靠。

在科学研究和理论构建中，奥卡姆剃刀原则经常被用作一个启发式的工具，帮助科学家们筛选和评估不同的理论。它提醒研究者避免过度拟合理论，即避免构建过于复杂、包含许多不必要的假设的理论。

然而，奥卡姆剃刀原则并不是一个决定性的原则，而是一个指导性的原则。有时，更复杂的理论可能因为提供了更好的预测或者更全面地解释了现象而被接受。在这种情况下，额外的复杂性是有充分理由的，因此符合奥卡姆剃刀原则的精神。

## 经验风险是什么

经验风险（Empirical Risk）是机器学习中用来评估模型性能的一个概念。它指的是模型在训练数据集上的平均损失。具体来说，经验风险是通过对训练数据集中的每个样本应用损失函数（例如平方损失、交叉熵损失等）来计算的，然后取所有样本损失的平均值。

经验风险是模型训练过程中的一个关键指标，因为它直接关联到模型在训练数据上的表现。最小化经验风险是训练模型的一个主要目标，这通常通过优化算法（如梯度下降）来实现。

然而，经验风险最小化并不总是能保证模型在未见过的数据（即测试数据）上表现良好。这是因为模型可能会过拟合训练数据，即模型学习到了训练数据中的噪声和特异性，而没有捕捉到数据的真实分布。因此，除了经验风险，机器学习中还会考虑其他指标，如泛化误差（Generalization Error），来评估模型的泛化能力。

## 预训练模型的输出维度和下游任务所需的维度不匹配，应该怎么解决

当预训练模型的输出维度与下游任务所需的维度不匹配时，有几种方法可以处理这种维度不一致的问题：

1. **增加全连接层（Dense Layer）** ：
   - 在预训练模型之后添加一个或多个全连接层，最后一个全连接层的输出维度设置为1024。这样可以有效地将512维的输出转换为1024维。
   - 例如，可以添加一个512到1024的线性层，并可选地在这之前添加一些非线性激活层（如ReLU）。
2. **维度扩展** ：
   - 使用某种形式的维度扩展技术，如重复（Repetition）、复制（Duplication）或插值（Interpolation），将512维的数据扩展到1024维。
   - 例如，可以将512维向量复制一次，然后连接起来形成1024维向量。
3. **特征映射** ：
   - 设计一个映射函数，将512维的特征映射到1024维。这可以通过学习一个矩阵来完成，该矩阵将512维输入转换为1024维输出。
4. **特征融合** ：
   - 如果下游任务有额外的输入或特征，可以考虑将这些特征与预训练模型的输出进行融合，以达到所需的1024维。

在实际操作中，通常会选择增加全连接层的方法，因为它是最灵活且在神经网络中常见的方法。这种方法可以通过训练过程中的反向传播自动学习如何最好地将512维的特征转换为1024维。其他方法可能需要更复杂的定制或额外的假设。无论选择哪种方法，都应该在下游任务的验证集上评估其效果，以确保模型的性能不受影响。

## 讲一下AUC，最好的AUC曲线应该是什么样子

AUC（Area Under Curve），即曲线下面积，是评估分类模型性能的重要指标，特别是在处理不平衡数据集时非常有用。AUC通常与ROC（Receiver Operating Characteristic）曲线一起使用。

### ROC曲线

ROC曲线是一种图形工具，用于评价二分类模型的性能。它通过显示真阳性率（True Positive Rate，TPR）与假阳性率（False Positive Rate，FPR）之间的权衡来帮助我们理解模型的分类能力。在二分类问题中，模型会预测每个样本属于正类的概率，然后根据这些概率来绘制ROC曲线。

### AUC

AUC是ROC曲线下的面积，其值域为0,10,1。AUC值提供了衡量分类模型整体表现的定量指标：

- **AUC=1**：表示完美分类器，所有正例都排在负例前面。
- **0.5<AUC<1**：优于随机猜测，但仍有提升空间。
- **AUC=0.5**：与随机猜测相同，模型没有预测价值。
- **AUC<0.5**：比随机猜测还差，这种情况很少见。

### 最好的AUC曲线

理想的AUC曲线应该是紧贴着左上角，即ROC曲线尽可能地接近于坐标轴的左上角。这意味着模型能够非常有效地将正样本排在负样本前面，从而具有很高的分类性能。

### ROC和AUC的优势

1. **不依赖于类别分布** ：ROC曲线和AUC值不受正负样本比例的影响，因此在处理不平衡数据集时特别有用。
2. **直观展示模型性能** ：ROC曲线可以直观地展示模型在不同阈值下的性能。
3. **易于比较不同模型** ：通过比较不同模型的ROC曲线和AUC值，可以快速评估和比较模型的性能。

综上所述，AUC和ROC曲线是评估和优化分类模型性能的重要工具，特别是在处理不平衡数据集时。

## BERT 和 RoBERTa 有什么区别

BERT（Bidirectional Encoder Representations from Transformers）和RoBERTa（Robustly optimized BERT approach）都是基于Transformer架构的预训练语言模型，它们在很多自然语言处理（NLP）任务中都取得了显著的效果。以下是BERT和RoBERTa之间的一些主要区别：

1. **预训练任务** ：
   - **BERT**：使用了Masked Language Model（MLM）和Next Sentence Prediction（NSP）两个预训练任务。MLM任务涉及随机掩盖输入文本中的某些词，并要求模型预测这些词；NSP任务则是预测两个句子是否是连续的。
   - **RoBERTa**：去掉了NSP任务，只使用了MLM任务。RoBERTa的作者发现NSP任务对于模型性能的提升并不显著，而且可能会引入噪声。
2. **训练数据量** ：
   - **BERT**：使用了英文维基百科（约2500万篇文章，1100亿个词）和书籍语料库BookCorpus（约1300万个词）进行预训练。
   - **RoBERTa**：使用了更大的数据集，包括英文维基百科的全部内容（约3800万篇文章，1350亿个词）和额外的CommonCrawl数据（约60亿个网页）。
3. **动态掩码** ：
   - **BERT**：在预训练数据准备阶段，掩码策略是静态的，即一旦掩码，在整个训练过程中保持不变。
   - **RoBERTa**：引入了动态掩码策略，即在每个训练epoch中都会随机生成不同的掩码，这样可以让模型学习到更加鲁棒的语言表示。
4. **训练批次大小和步数** ：
   - **BERT**：使用了较小的批次大小和较少的训练步数。
   - **RoBERTa**：使用了更大的批次大小和更多的训练步数，这有助于模型在预训练过程中捕捉到更多的语言特征。
5. **文本编码** ：
   - **BERT**：在处理输入文本时，使用了WordPiece分词器，并将文本分割成更小的token。
   - **RoBERTa**：同样使用WordPiece分词器，但是对BERT的文本编码策略进行了一些改进，例如，不再使用句子A和句子B的区分标记，而是**使用更多的分隔符来处理长文本**。
6. **模型变种** ：
   - **BERT**：发布了多个不同大小的模型，如BERT-Base和BERT-Large。
   - **RoBERTa**：也发布了不同大小的模型，并且在某些任务上性能优于同等大小的BERT模型。

总的来说，RoBERTa可以看作是BERT的一个改进版本，它在预训练数据量、训练策略和模型细节上进行了优化，从而在很多NLP任务上取得了更好的性能。

## 讲述一下Transformer模型中注意力机制的演进

MHA（**M**ulti-**H**ead **A**ttention），也就是多头注意力，是开山之作[《Attention is all you need》](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/139034599)所提出的一种 Attention 形式，可以说它是当前主流 LLM 的基础工作。在数学上，多头注意力 MHA 等价于多个独立的单头注意力的拼接。

而后面的 MQA、GQA、MLA，都是围绕“如何减少 **KV Cache** 同时尽可能地保证效果”这个主题发展而来的产物。

### KV Cache

LLM推理的过程是一个自回归的过程，也就是说前i次的token会作为第i+1次的预测数据送入模型，拿到第i+1次的推理token。在这个过程中Transformer会执行自注意力操作，为此需要给当前序列中的每个项目（无论是prompt/context还是生成的token）提取键值（kv）向量。这些向量存储在一个矩阵中，通常被称为kv cache。kv cache是为了避免每次采样token时重新计算键值向量。利用预先计算好的k值和v值，可以节省大量计算时间，尽管这会占用一定的存储空间。

### Page Attention

PageAttention命名的灵感来自OS系统中虚拟内存和分页的思想。可以实现在不连续的空间存储连续的kv键值。

所有键值都是分布存储的，需要通过分页管理彼此的关系。序列的连续逻辑块通过 block table 映射到非连续物理块。

同一个prompt生成多个输出序列，可以共享计算过程中的attention键值，实现copy-on-write机制，即只有需要修改的时候才会复制，从而大大降低显存占用。

### MHA/GQA/MQA 优化技术

<img src="https://i-blog.csdnimg.cn/blog_migrate/8b142142551b19232abc102f35d67601.png" alt="img" style="zoom:80%;" />

#### MQA (Multi-Query Attention)

**概述**：MQA提出了一种混合的方式来处理查询、键和值，从而在减少计算量的同时，保持模型的表现。

**工作原理**：

- MQA通过减少查询头的数量（即减少Q的投影次数），但允许每个查询头仍然和多个键和值进行交互。也就是说，**多个注意力头共享查询子空间的表示**。
- 这种机制在降低计算复杂度的同时，仍然可以捕捉到多个子空间的信息。

使用 MQA 的模型包括 PaLM [6]、StarCoder [7]、Gemini [8] 等。

#### GQA (**G**rouped-**Q**uery **A**ttention)

GQA通过减少注意力头的数量，但保持注意力头之间的多样性，来降低计算复杂度。

**工作原理**：

- 在GQA中，多个查询头共享一个键和值的子空间。这意味着不同的查询头关注的是同一组键和值，而不是像MHA那样完全独立。
- 通过这种共享机制，减少了独立投影所需的计算量，同时仍然保留了多样性。

GQA 提供了 MHA 到 MQA 的自然过渡。Meta 开源的 LLAMA2-70B [10]，以及 LLAMA3 [11] 全系列，ChatGLM系列使用的是GQA。

#### MLA (Multi-head Latent Attention)

> 似乎是在DeepSeek-v2中用到的

在标准的MHA结构中，每个token的query、key和value通过参数矩阵映射得到，并分割成多个注意力头。每个头独立计算注意力权重并得到输出，这个过程虽然能捕捉丰富的上下文信息，但在推理时需要缓存大量的KV Cache。

为每个注意力头使用独特的投影矩阵来增强GQA的能力，同时仍与MHA共享相同的键-值（KV）缓存大小。（MLA通过对keys和values进行低秩联合压缩来降低KV Cache）

### MLA 怎么与 RoPE (旋转位置编码) 做结合

接下来 MLA 讨论的一个问题是，在上面的压缩过程中我们并没有考虑到 RoPE。原始的 RoPE 需要在 query 和 key 中融入相对位置信息。在 MLA 中，在 query 中融入相对位置信息是比较容易的，但是由于 KV Cache 缓存的是压缩后的低秩 key-value 信息，这里面是没办法融入相对位置信息的。这就意味着，推理时我们必须重新计算所有之前 tokens 的 keys，这将大大降低推理效率。

![image-20240819180233391](C:\Users\j50043562\AppData\Roaming\Typora\typora-user-images\image-20240819180233391.png)

## Flamingo 的具体实现

Flamingo模型的核心思想是将预训练好的视觉特征编码器（如ViT、Resnet等）和大语言模型进行结合。视觉特征编码器负责将图像信息转换成特征向量，而大语言模型则负责将这些特征向量与文本信息进行交织和融合。通过这种方式，Flamingo模型可以实现对图像和文本信息的[联合建模](https://cloud.baidu.com/product/dsad.html)，从而在处理多模态信息时具有更强的泛化能力和鲁棒性。

在具体实现上，Flamingo模型采用了**感知重采样技术**和**门控交叉注意力技术**。Flamingo的视觉编码器和LLM都是固定参数，不在训练中更新。感知重采样器将变长的视觉向量转换成定长的多模态语义向量，而门控交叉注意力技术则负责将这些多模态语义向量与文本信息进行融合。通过这种方式，Flamingo模型可以在输入中混合多模态信息，并输出相应的文本信息。

其中的视觉编码器采用NFNet（NormalizerFree ResNet），作者先在图文对数据上采用CLIP的方式对NFNet进行预训练，随后进行参数固定。如果视觉端输入是视频，则按照1 fps进行采样后将N NN帧进行视觉特征提取（若是图片输入，则N=1），注意到此时position embedding按照帧粒度组织，即是统一帧的不同patch共用一个position embedding以建模帧间序列信息。尔后对多帧的特征进行展开、拼接，作为transformer的k和v，而采用一个可学习的query向量作为transformer的q输入，感知重采样机制的一个好处就是，可以将**变长的视频输入转变为定长的输入，此处定长的输入长度为64。**

门控注意力单元的设计，则是在原先固定的LLM结构的每一层基础上叠加了门控单元，门控单元由交叉注意力机制和门控结构、FFW交替组成，其中交叉注意力的k和v都是感知重采样器的输出，而q则是文本输入。为了保证在训练初始阶段模型和原先的LLM不至于偏差太远，作者采用了门控机制。

## Pre-Norm 和 Post-Norm 有什么区别

Bert的Post-Norm，是在Add操作后进行Norm操作，因此叫做Post-Norm。而Pre-Norm则是Norm之后再Add，所以叫Pre-Norm。

简单说就是Post-Norm由于是在残差之后进行归一化，因此归一化的效果更好，使得模型的**鲁棒性更强**。

而Pre-Norm由于并不是所有的参数都参与正则化，因此整体来说更**不容易发生梯度消失**的问题，模型训练的稳定性更强。

因此，在Bert时代由于层数较浅，往往采用的是Post-Norm，而到了大模型时代，由于transformer的层数开始加深，为了训练稳定性开始使用Pre-Norm。

### 扩展：DeepNorm

作者认为，出现训练不稳定（梯度消失/爆炸）的主要原因是**同时更新的参数过多（爆炸式更新）。**

> 有一种技巧是 warmUp 即刚开始使用更小的学习率进行训练，然后慢慢增大训练

Post-LN-no warmup在一开始更新了过多的参数，导致模型错误的进入了一个局部最优解。由于LN的梯度大小与LN的输入成反比，因此当更新的参数量过大时，会使LN的输入变大，因此这也导致了LN梯度逐渐消失，使得模型很难脱离出局部最优解，进一步破坏了算法的稳定性。而Post LN init或者warmup更新的参数很小，LN相对稳定，减轻了梯度消失的可能性，使得训练更加稳定。 

DeepNorm主要包括两部分，一个是模型初始化，另一个是对Post-Norm的改进。研究者在残差连接处引入了一个新的归一化函数 —— DEEPNORM，它从理论上保证了把模型更新过程限制为常数。

## BERT 有多少层

- **BERT-base** 版本有 12 层（transformer 层），每层有 768 个隐藏单元，模型参数总数约为 1.1 亿。
- **BERT-large** 版本有 24 层，每层有 1024 个隐藏单元，模型参数总数约为 3.4 亿。

## 位置编码相关问题

### Transformer 中为什么要有位置编码

在Transformer模型中，使用位置编码（Position Encoding）是为了提供序列中单词的位置信息，因为Transformer架构本身是完全基于自注意力机制的，没有内置的顺序信息。具体来说，位置编码的必要性可以归纳为以下几点：

1. **无序性** ：传统的循环神经网络（RNN）和卷积神经网络（CNN）能够通过其结构自然地处理序列数据的顺序。RNN通过时间步的顺序处理输入，而CNN通过局部感受野捕捉上下文信息。而Transformer使用自注意力机制，使得输入序列中的每个元素可以在任何位置直接与其他元素进行交互，这种机制本质上是无序的。
2. **捕捉位置信息** ：为了让模型理解单词在句子中的相对位置，位置编码被添加到输入的词向量中。这样，模型就能够区分“我爱你”和“你爱我”这样的句子，因为它们的单词顺序不同。
3. **相对位置** ：位置编码通常是通过正弦和余弦函数生成的，这样可以产生不同频率的编码，使得模型能够捕捉到不同位置之间的相对距离。这种设计使得模型在处理长序列时能够更好地理解相对位置。
4. **平滑性** ：通过使用连续的正弦和余弦函数，位置编码在不同位置之间提供了一种平滑的过渡，这有助于模型的学习过程。

### BERT的位置编码和Transformer的位置编码有什么区别

- **Transformer**：原始的Transformer模型（如Vaswani等人在2017年提出的模型）使用**正弦和余弦函数** 进行位置编码。这种编码是固定的，且不依赖于输入的数据内容。具体来说，位置编码是基于序列位置生成的，通过将每个位置映射为不同频率的正弦和余弦值。这种编码方式使得模型能够感知序列中不同位置的信息。
- **BERT**：BERT采用的是**可训练的嵌入（embedding）** 作为位置编码。这意味着，位置编码不是基于固定的函数生成，而是通过训练过程中的反向传播进行调整和优化。这样，BERT可以学习到更加适应特定任务的位置信息编码。

### ROPE 位置编码：为什么它比绝对或相对位置编码更好?

旋转位置嵌入是最先进的 NLP 位置嵌入技术。大多数流行的大型语言模型（如 Llama、Llama2、PaLM 和 CodeGen）已经在使用它。

相对位置编码有一些问题：

1. 计算效率低下：必须创建成对的位置编码矩阵，然后执行大量张量操作以获得每个时间步的相对位置编码。特别是对于较长的序列。这主要是由于自注意力层中的额外计算步骤，其中**位置矩阵被添加到查询键矩阵中**。
2. 键值缓存使用的复杂性：由于每个附加令牌都会改变每个其他令牌的嵌入，这使得 Transformer 中键值缓存的有效使用变得复杂。使用 KV 缓存的一项要求是**已经生成的单词的位置编码，** **在生成新单词时**不改变（绝对位置编码提供）因此相对位置编码不适合推理，因为**每个标记的嵌入会随着每个新时间步的变化而变化。**

RoPE  代表了一种编码位置信息的新方法。传统方法中无论是绝对方法还是相对方法，都有其局限性。绝对位置编码为每个位置分配一个唯一的向量，虽然简单但不能很好地扩展并且无法有效捕获相对位置；相对位置编码关注标记之间的距离，增强模型对标记关系的理解，但使模型架构复杂化。

RoPE巧妙地结合了两者的优点。允许模型理解标记的绝对位置及其相对距离的方式对位置信息进行编码。这是通过**旋转机制**实现的，其中序列中的每个位置都由嵌入空间中的旋转表示。RoPE 的优雅之处在于其简单性和高效性，这使得模型能够更好地掌握语言语法和语义的细微差别。

旋转矩阵的原理

![img](https://developer.qcloudimg.com/http-save/yehe-7220647/98b4cc9e928f97cef615789c02e2da3a.jpg)

RoPE 引入了一个新颖的概念。它不是添加位置向量，而是对词向量应用旋转。旋转角度 (θ) 与单词在句子中的位置成正比。第一个位置的向量旋转 θ，第二个位置的向量旋转 2θ，依此类推。这种方法有几个好处：

1. **向量的稳定性：**在句子末尾添加标记不会影响开头单词的向量，有利于高效缓存。
2. **相对位置的保留：**如果两个单词在不同的上下文中保持相同的相对距离，则它们的向量将旋转相同的量。这确保了角度以及这些向量之间的点积保持恒定

![img](https://developer.qcloudimg.com/http-save/yehe-7220647/0985c92b08ed83dd0545522bd93fca4b.jpg)

## Transformer中为什么要scale？为什么scale的数值是 sqrt(dk)  

- 均值为 0 ⽅差为 1 的独⽴随机变量的内积⽅差为维度⻓度 dk ，所以除标准差 sqrt(dk) ◦ 
- 如果不scale，随着层数的加深，这个⽅差会越来越⼤，最终出现极⼤值，使softmax梯度消失 （softmax输⼊的数字量级越⼤，导数也越接近0） ◦ 
- QK的乘积太⼤，softmax之后每⼀个位置的梯度太⼩了，所以需要除⼀个scale值来放缩

## Transformer 为什么要使用 layer norm 不使用 batch norm

在Transformer模型中，使用Layer Normalization（层归一化）而不是Batch Normalization（批归一化）有几个关键原因：

1. **序列长度的变化** ：在自然语言处理任务中，输入序列的长度可能会有很大变化。Batch Normalization依赖于批量中的样本数量，如果批量大小不一致，会导致归一化效果不稳定。而Layer Normalization是对每个样本独立进行归一化，不受序列长度变化的影响。
2. **小批量问题** ：在某些情况下，批量大小可能很小，这会导致Batch Normalization的统计特性不准确。Layer Normalization不依赖于批量大小，因此在小批量情况下表现更稳定。
3. **训练和推理的一致性** ：Batch Normalization在训练和推理时使用不同的统计特性（训练时使用批量统计，推理时使用全局统计），这可能导致训练和推理之间的不一致性。Layer Normalization在训练和推理时使用相同的归一化方式，保持了一致性。
4. **并行化能力** ：Layer Normalization可以更容易地在不同的序列上并行化，因为它不依赖于批量中的其他样本。这对于Transformer模型中的自注意力机制尤为重要，因为自注意力机制本身就是高度并行的。
5. **对初始化的敏感性** ：Batch Normalization对权重初始化较为敏感，而Layer Normalization在这方面更为稳健。

综上所述，Layer Normalization在Transformer模型中提供了更好的稳定性、一致性和并行化能力，使其成为更合适的选择。

## Pre-Norm 和 Post-Norm 有什么区别

原版Transformer和BERT使用了 Post Normalization
为什么使用 Post Normalization ：残差会进一步放大方差，所以我们也要想相应的策略缩小其方差。
一个朴素的想法是在残差后面加一个 Layer Normalization 。但是这种方法会削弱 **残差的恒等分支** ，即削弱本来流向 x 的梯度，从而弱化了残差 **易于训练** 的优点。

Post-Normalization（后归一化）在训练深度学习模型时可能更加难以训练，主要是因为它的归一化过程是在每个层的输出之后进行的，这可能导致以下几个问题：

1. **梯度消失或爆炸** ：后归一化可能会使得梯度在反向传播时更容易消失或爆炸，尤其是在深层网络中。这是因为在每一层的输出之后进行归一化，可能会影响到梯度的传播，使得模型的训练变得更加不稳定。
2. **学习率敏感性** ：后归一化使得模型对学习率的选择更加敏感。如果学习率设置得不合适，模型可能会在训练初期表现得不稳定，导致收敛困难。
3. **训练初期的不稳定性** ：在训练的初期，模型的参数可能还没有得到良好的初始化，导致输出的分布不稳定。此时进行归一化可能会导致输出值的剧烈波动，从而影响训练过程。

为了应对这些问题，通常会采用“warm-up”策略，即在训练的初期使用较小的学习率，逐渐增加到预设的学习率。这种策略可以帮助模型在训练初期稳定下来，避免因过大的学习率导致的训练不稳定，并且有助于模型逐渐适应后归一化带来的影响。

而Pre-Norm则是Norm之后再Add，所以叫Pre-Norm。Pre Norm 在训练稳定和收敛性方面有明显的优势。

## Self Attention 的时间复杂度计算

 ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c570326ecf9ce41a1a12c9b925b0f069.png#pic_center)

![image-20240826104640332](C:\Users\j50043562\AppData\Roaming\Typora\typora-user-images\image-20240826104640332.png)

## 大模型精度问题（FP16，FP32，BF16）

### FP16

FP16也叫做 float16，两种叫法是完全一样的，全称是Half-precision floating-point(半精度浮点数)，在IEEE 754标准中是叫做binary16，简单来说是用16位二进制来表示的浮点数

![img](https://pic1.zhimg.com/80/v2-2babbab55e47c4669e5e23e0f01df7d8_720w.webp)

- Sign(符号位): 1 位，0表示整数；1表示负数。
- Exponent(指数位)：5位，简单地来说就是表示整数部分，范围为00001(1)到11110(30)，正常来说整数范围就是 21−230 ，但其实为了指数位能够表示负数，引入了一个偏置值，偏置值是一个固定的数，它被加到实际的指数上，在二进制16位浮点数中，偏置值是 15。这个偏置值确保了指数位可以表示从-14到+15的范围即 2−14−215 ，而不是1到30，注：当指数位都为00000和11111时，它表示的是一种特殊情况，在IEEE 754标准中叫做非规范化情况，后面可以看到这种特殊情况怎么表示的。
- Fraction(尾数位)：10位，简单地来说就是表示小数部分，存储的尾数位数为10位，但其隐含了首位的1，实际的尾数精度为11位，这里的隐含位可能有点难以理解，简单通俗来说，假设尾数部分为1001000000，为默认在其前面加一个1，最后变成1.1001000000，换成10进制有两种方式

> ```python
> # 第一种计算方式
> 1.1001000000 = 1 * 2^0 + 1 * 2^(-1) + 0 * 2^(-2) + 0 * 2^(-3) + 1 * 2^(-4) + 0 * 2^(-5) + 0 * 2^(-6) + 0 * 2^(-7) + 0 * 2^(-8) + 0 * 2^(-9) = 1.5625
> # 第二种计算方式
> 1.1001000000 = 1 + 576(1001000000变成10进制)/1024 = 1.5625
> ```

FP16(float16)表示的范围[-65504，65504]。

![img](https://pic4.zhimg.com/80/v2-c12538540c3ae7e509944b4adf94924b_720w.webp)

#### Normal Number（规范数）

规范数是浮点数表示中一种标准形式。它的特点是数值的有效位（即尾数部分）在规定的范围内，通常是以 1.xxxx 的形式表示（在二进制中）。在 IEEE 754 标准中，规范数的指数部分是经过偏移的

#### Subnormal Number（非规范数）

非规范数（或称为次规范数）是浮点数的一种特殊情况，其有效位不以 1 开头，而是以 0 开头。

接下来看一下在pytorch中是如何表示的：

```pytho
torch.finfo(torch.float16)
finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)
```

1. eps

- **定义** ：`eps` 是指浮点数表示的最小正数，它与 1.0 的差值是可区分的最小值。也就是说，`eps` 是 FP16 中可以表示的最小的正数，使得 1.0 和 (1.0 + eps) 之间的差值可被识别。
- **用途** ：用于数值稳定性和避免除以零等问题。

2. resolution

- **定义** ：`resolution` 是指在 FP16 表示中最小的可区分的增量。它通常等同于 `eps`，表示 FP16 能够分辨的最小数值变化。
- **用途** ：用于确定浮点数计算的精度，尤其是在数值计算的上下文中。

3. tiny

- **定义** ：`tiny` 是指 FP16 中的最小非零值，通常是次规范数（subnormal number）中的最小值。它表示 FP16 能够表示的最小的非零数，但不是规范数。
- **用途** ：用于表示非常小的数值，尤其在需要很小的数值时，例如在某些数值算法中。

4. smallest_normal

- **定义** ：`smallest_normal` 是指 FP16 中的最小规范数（normal number）。这是 FP16 能够表示的最小的规范数。
- **用途** ：用于确定 FP16 能够表示的最小的有效数值，通常用于浮点数运算的范围界限。

### BP16

BF16也叫做bfloat16(这是最常叫法)，其实叫“BF16”不知道是否准确，全称brain floating point。和上述FP16不一样的地方就是指数位和尾数位不一样，看图：

![img](https://pic3.zhimg.com/80/v2-864431d6759a840c7414cb878d0213de_720w.webp)

```python
import torch
torch.finfo(torch.bfloat16)
# 结果
finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)
```

相比于 FP16，表示的精度降低了，但是表示的范围增加了，可以防止在训练过程中的溢出。

### FP32

也叫做 float32，使用了32位二进制来表示浮点数。

![img](https://pic4.zhimg.com/80/v2-25c334d391305cc5c597e0d3ed48940f_720w.webp)

> 一般来说 fp16的收敛速度要快一些，bf16用了更多的位来表示指数部分，留下了较少的位数用于表示有效数字，表示范围变大了，但更多的指数部分增加了计算和存储的开销

### KV Cache 的占用字节数分析

![image-20240826115042939](C:\Users\j50043562\AppData\Roaming\Typora\typora-user-images\image-20240826115042939.png)

## 数据清洗和处理有哪些手段

1. 使用传统的脚本设置正则化过滤，筛选有用数据，清除脏数据

​	数据清洗的方法包括选择高质量数据源，规范化文本、去除HTML和标记、过滤停用词和噪声词，处理拼写错误、清除敏感信息，检测和处理异常值，评估数据质量，并记录清洗过程。

这些步骤有助于提高模型的准确性、稳定性和安全性，确保生成的输出更加可靠和高质量。

2. 借助AI模型进行清洗。这涉及使用机器学习技术来培训智能体，使其能够自动识别和清洗数据，以优化人工与机器在数据清洗中的工作分配。
   贝叶斯分类算法也被用于数据清洗，这是一种利用概率统计知识进行分类的算法。
   还有一些尝试使用文本识别算法和识别技术等AI能力进行数据清洗。例如，决策树和随机森林算法具备根据特征辨别不良数据的能力。经典的ChatGPT训练数据通过OpenAI专门训练了一个过滤模型，用于识别有害内容，以确保对模型的输入和输出数据进行有效管控，以遵守使用政策。
3. 人工审查

##  基座大模型结构解析

### ChatGLM 升级之路

#### ChatGLM

使用了 Prefix-LM

ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型。ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。

#### ChatGLM2

有如下新特性

1. 使用了更大的数据集
2. 基于 FlashAttention 技术扩充了基座模型的上下文长度（2k -> 32k）
3. 基于 Multi-Query Attention 技术，有更高效的推理速度和更低的显存占用

升级过程：

1. 模型结构 从Prefix-LM回归纯粹的Decoder-Only结构

​	为模型的**训练效率**带来了极大的提升。

2. 序列长度

​	预训练在32k数据集进行，微调在8k数据集进行

3. 算子优化

​	Flash Attention  Multi-Query Attention (MQA)

**FlashAttention**

计算复杂度和序列长度的平方![N^2](https://latex.csdn.net/eq?N%5E2)成正比，可以看一个小例子，比如两个相乘的矩阵大小分别为(![N \times d](https://latex.csdn.net/eq?N%20%5Ctimes%20d)) 和(![d \times N](https://latex.csdn.net/eq?d%20%5Ctimes%20N))，矩阵乘法的一种计算方式是使用第一个矩阵的每一行与第二个矩阵的每一列做点乘。因为我们需要拿第一个矩阵的每一行去与第二个矩阵的每一列做点乘，所以总共就需要 ![N^2](https://latex.csdn.net/eq?N%5E2) 次点乘。而每次点乘又需要 ![d](https://latex.csdn.net/eq?d) 次乘法，所以总复杂度就为 O(N2d)

Self-Attention 的计算复杂度

<img src="C:\Users\j50043562\AppData\Roaming\Typora\typora-user-images\image-20240826161155706.png" alt="image-20240826161155706" style="zoom:80%;" />

MLP 层的计算复杂度

![image-20240826161425651](C:\Users\j50043562\AppData\Roaming\Typora\typora-user-images\image-20240826161425651.png)

Logits 的计算量

![image-20240826161711502](C:\Users\j50043562\AppData\Roaming\Typora\typora-user-images\image-20240826161711502.png)

**标准注意力Standard Attention的两个问题：显存占用多、HBM读写次数多**

> 尽管已经有许多近似注意力的方法尝试减少attention的计算和内存要求。例如，稀疏近似和低秩近似的方法，将计算复杂度降低到了序列长度的线性或亚线性
> 但这些近似注意力方法方法并没有得到广泛应用。因为这些方法过于关注FLOPS(浮点数计算次数)的减少，而忽略了IO读写的内存访问开销，导致这并没有效减少运行时间(wall-clock time)
> 总之，在现代GPU中，计算速度已经远超过了显存访问速度，transformer中的大部分计算操作的瓶颈是显存访问。对于显存受限的操作，IO感知是非常重要的，因为显存读写占用了大部分的运行时间

总之，transformer的核心组件self-attention块的计算复杂度和空间复杂度是序列长度 的二次方
且对于self-attention块，除了两个大矩阵乘法是计算受限的，其他都是内存受限的逐点运算( 例如mask操作、softmax操作、dropout操作，这些逐点操作的性能是受限于内存带宽的，会减慢运行时间)

**Memory-efficient Attention：把显存复杂度从平方降低到线性，但HBM访问次数仍是平方**

在注意力计算过程中，节省显存的主要挑战是softmax与![K,V](https://latex.csdn.net/eq?K%2CV)的列是耦合的，其方法是**单独计算softmax的归一化因子**，来实现解耦。被称作 lazy softmax

**Flash Attention：通过kernel融合降低HBM读写次数，避免频繁地从HBM中读写数据**

对于性能受限于内存带宽的操作，进行加速的常用方式就是kernel融合，该操作的典型方式分为三步：

1. 每个kernel将输入数据从低速的HBM中加载到高速的SRAM中
2. 在SRAM中，进行计算
3. 计算完毕后，将计算结果从SRAM中写入到HBM中

但是SRAM的内存大小有限，不可能一次性计算完整的注意力，因此必须进行分块计算，使得分块计算需要的内存不超过SRAM的大小。矩阵乘法的分块计算比较容易实现，难点在于Softmax的计算。因为计算归一化因子时，需要获得到完整的输入数据（Safe Softmax）。

> 为什么计算归一化因子时需要完整的输入数据呢，因为在深度学习中，为了避免发生的数值溢出的问题，计算时同窗会减去最大值，成为“Safe Softmax”

那到底怎么解决**分块计算的难点——softmax的分块计算**呢？考虑到softmax与 ![K](https://latex.csdn.net/eq?K) 的列是耦合的，故可以通过引入了两个额外的统计量 m(x),l(x) 来进行解耦(**前者类似最大分数，后者类似exp分数总和\**\)，实现了分块计算

![image-20240827110224452](C:\Users\j50043562\AppData\Roaming\Typora\typora-user-images\image-20240827110224452.png)

### LLama 升级之路

#### LLama2 

在模型结构上，主要升级两点：

1. 训练数据Token数量从 1.4T -> 2T
2. 序列长度从2k -> 4k

在**预训练**过程中，**更强大的数据清洗，更新数据组合，增加40%的总训练tokens，加倍上下文长度，以及使用分组查询注意力（GQA）来提高更大模型的推理可扩展性**。

在 **SFT** 过程中，LLAMA2强调数据质量的重要性，通过2W的高质量指令数据，激发模型的指令遵循能力。

在**RLHF过程**中，LLAMA2做了较多工作，对RLHF过程作出了进一步的解释。**自建了100W的Reward数据集**，训练了两个独立的Reword Model（Helpfulness RM / Safety RM）。

**迭代训练**

LLAMA2采用了两种强化学习算法:PPO和拒绝采样算法。

这两种强化学习算法主要区别在于：

• 广度：在拒绝采样中，模型为给定的提示探索K个样本，而在PPO中，只有一个生成过程。

• 深度：在PPO中，训练过程中第t步的样本是经过t-1步梯度更新后的模型策略的函数。在拒绝采样微调中，在模型的初始策略下采样所有输出以收集新数据集，然后类似于SFT进行微调。然而，由于采用了**迭代模型更新**，这两种算法之间的本质区别并不明显。

LLAMA2直到RLHF (V4)，仅使用拒绝采样微调。之后将这两种方法结合起来，先对拒绝采样检查点应用PPO，然后再对采样进行拒绝采样。LLAMA2只使用最大的70B Llama 2-Chat模型进行拒绝采样。其他较小的模型则在更大模型的拒绝采样数据上进行微调，从而将大模型的能力转移到较小的模型中。

### 百川升级之路

#### baichuan-7b

百川模型结构与LLAMA相近，作了如下的优化：

**分词器**

参考学术界方案使用 SentencePiece 中的 Byte-Pair Encoding (BPE) 作为分词算法，并且进行了以下的优化：

- 使用 2000 万条以中英为主的多语言语料训练分词模型，显著提升对于中文的压缩率。
- 对数字的每一位单独分开，避免出现数字不一致的问题，对于提升数学能力有重要帮助
- 对于罕见字词（如特殊符号等），支持 UTF-8 characters 的 byte 编码，因此做到未知字词的全覆盖。

**算子优化**

采用更高效的算子:Flash-Attention，同ChatGLM2

#### baichuan-13B

1. 参数量:baichuan13B较baichuan7B 首先在**参数量**上翻了一倍，更大的参数量意味着知识的容量更大，通过更多的训练数据(1.2T->1.4T)，基座模型的常识能力得以提升；
2. 位置编码:从RoPE改成ALiBi，在一定程度的可以进行长度外推(TIPS:RoPE可以进行更长范围的外推)；

#### baichuan2

**Positional Embeddings**：

- 对于Baichuan 2-7B模型，采用了Rotary Positional Embedding (RoPE)。
- 对于Baichuan 2-13B模型，采用了ALiBi作为位置编码技术。

**激活函数和归一化**：

- 使用了SwiGLU激活函数，这是GLU的一个变体，经过改进的版本。
- 在注意力层中采用了内存高效的注意力机制。

**Tokenizer**：

- 对词汇表的大小进行了调整，将其从Baichuan 1的64,000扩展到125,696，以在计算效率和模型性能之间取得平衡。

**NormHead：**

- Baichuan 2使用了一种称为NormHead的方法来稳定训练并提高模型性能。NormHead主要用于对输出嵌入进行归一化处理，有助于稳定训练动态，并降低了L2距离在计算logits时的影响。

**最大z损失（Max-z loss）**：

- 引入了最大z损失，用于规范模型输出的logit值，从而提高训练的稳定性并使推断更加鲁棒。

## 如何构建一个好的基座大模型

