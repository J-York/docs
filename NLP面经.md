NLP 面经
NLP 常见任务
文本分类、情感分析、机器翻译、文本摘要、文本生成、问答对话系统、语音识别等

NLP 常见模型
循环神经网络 RNN，Transformer 模型，RNN 有两种变体 LSTM 和 GRU。

循环神经网络
循环神经网络（Recurrent Neural Networks, RNN）是一种用于处理序列数据的神经网络，其特点是能够在序列的每个时间步上保持一个隐藏状态，并将其传递给下一个时间步。

RNN的基本模型结构

Pytorch中对RNNCell的定义

RNN 的长期依赖问题：简单来说就是由于 RNN 这种特殊的循环结构，每一步的状态都包含了上一个状态的输出，因此根据链式法则在反向传播求导的时候，越早的项连续相乘的部分就会越长。在处理长序列数据的时候，若连续相乘的项很小就有梯度消失的问题，而相反相乘项比较大的时候就会出现梯度爆炸的问题。

为了解决这个问题，出现了改进版本的RNN，如长短时记忆网络（LSTM）和门控循环单元（GRU），它们通过引入门机制来更好地控制信息的流动，从而增强对长距离依赖的建模能力。

长短期神经网络
长短期神经网络是 RNN 的一种，增加了多个门控结构，以及新增了一个信息流-------细胞状态。

LSTM模型结构

x_t 为输入向量

h_t为隐藏状态向量

C_t为细胞状态向量

黄色小框从左到右依次为：f 为遗忘门、i 输入门（中间俩小框）、o 输出门

遗忘门是决定了从记忆单元中丢弃多少就信息；

输入门也叫更新门，是对细胞状态向量的更新；

包含一个 Sigmoid 层决定哪些值将被更新

tanh （双曲正切）层生成新的候选值（新信息）

输出门则是由输入、隐藏以及细胞状态向量共同决定模型的输出。

LSTM通过这些特殊的门控结构和细胞状态引入，有效地控制和存储信息，有效地改善了长期依赖问题，在一定程度上缓解了梯度爆炸和梯度小时问题。但也因此产生了大量的参数，训练开销增大。

门控循环单元 （GRU）
门控循环单元是在LSTM（长短期记忆网络）的基础上提出的，旨在简化LSTM的结构并提高其计算效率。GRU的设计目标是减少LSTM的参数数量，从而降低计算复杂度，并提高模型的训练速度。

GRU 主要由两个门控机制组成：

重置门（Reset Gate）

决定过去的信息在当前的时间步的隐藏状态中保留多少

更新门（Update Gate）

决定了当前的隐藏状由过去信息和当前信息输入影响多少

GRU模型结构

GRU与LSTM的主要区别
结构简单性 ：

门控数量 ：GRU只有两个门（重置门和更新门），而LSTM有三个门（遗忘门、输入门和输出门）。这使得GRU的结构更为简单，参数更少。

记忆单元 ：LSTM具有单独的记忆单元状态，而GRU则直接输出隐藏状态，没有单独的记忆单元。

信息流动 ：

更新方式 ：在LSTM中，输入门和遗忘门共同作用来更新单独的记忆单元，然后通过输出门生成新的隐藏状态。而在GRU中，更新门直接控制信息的保留与更新，减少了信息流动的复杂度。

计算效率 ：

GRU因其更简单的结构，通常计算上比LSTM更高效，训练更快，尤其是在小数据集和较小模型中。

Transformer 模型
它是一种基于自注意力机制的深度神经网络模型，特别适用于处理序列数据。

Transformer整体架构

Multi-Head Attention： 这一块是Transformer模型的重点！多头注意力机制是一种扩展自注意力机制的方法，它将自注意力机制分解为多个“头”，每个“头”都在不同的表示空间中学习信息，从而能够捕捉到更丰富的特征和关系。具体的：

赋值： 经过位置编码的矩阵X分别赋值给V、K、Q，因此目前这三个矩阵一模一样且都等于X；

线性层Linear： 矩阵V、K、Q分别输入独立的线性层，进行线性变换，即各自乘上一个参数矩阵。参数矩阵是并行独立训练出来的，所以变换后V、K、Q现在不相同了，但他们维度还是一致的；

拆分： V、K、Q根据头数h进行拆分，多头进行并行工作。在实际代码实现中就是：比如一个3*512的V矩阵，拆分成8头，即变成了8 * 3 * 64的矩阵。这样达到的效果是后续的矩阵运算本质上是每头单独进行运算，而不是一整块进行运算

Scaled Dot-Product Attention: 经过拆分后的V、K、Q输入“缩放点乘积注意力”模块，这个模块是多头注意力机制中的核心模块，基于K、Q矩阵通过一番操作得到输入句子中每个词之间的注意力权重矩阵，然后对值矩阵（V）进行加权求和，从而使得每个词向量都融合上下文的信息，达到了语义增强的目的。该模块数学本质就是下面这个公式。

image-20240816154212651

掩码操作Mask(opt.)：Mask包含Padding Mask和Sequence Mask两种，此处Mask是指Sequence Mask。掩码就是让矩阵某些元素变为负无穷的数，使得其在后续Softmax中的概率为0。其中Padding Mask旨在消除输入序列中Padding的影响；模型图中Mask是指Sequence Mask，只存在于解码器中，目的是在预测下一个词时，覆盖住后面的词汇注意力信息，达到只用前面序列来预测下一词的目的。

Multi-Head Attention

相加 & 标准化：将Multi-Head Attention模块输入矩阵和输出矩阵相加，然后将其标准化；

前馈神经网络： 将矩阵输入一个两层的全连接网络，激活函数为ReLU。输出维度和输入保持一致；

BERT 模型
BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer Encoder的预训练语言处理模型

BERT模型的子模块有三个嵌入层

BERT的输入

Segment嵌入：仅在处理输入为句子对（NSP）问题时使用。其他任务均赋值为0

Position嵌入：与Transformer中基于函数的 PostionEncoding 方式不同，BERT 的位置嵌入是基于参数的方式，与词嵌入相似。

BERT 的训练分为两个阶段：预训练阶段与微调阶段

预训练阶段
在预训练阶段，BERT模型使用大量无标注文本进行训练，目的是学习通用的语言表示。

预训练阶段又主要包括两个任务：

Masked Language Model (MLM) ：

目的 ：通过训练模型去预测句子中被随机掩盖的单词，让模型学习到上下文信息。

方法 ：在输入文本中，随机选择15%的单词，将其中的80%替换成[mask]标记（如"[MASK]"），10%替换成随机单词，10%保持不变。模型的目标是根据上下文预测被掩盖的单词。这种方法使得模型能够建立双向的、上下文敏感的词表示。

Next Sentence Prediction (NSP) ：

目的 ：帮助模型理解句子间的关系，尤其是在处理问答和自然语言推理任务时。

方法 ：在预训练数据集中，有50%的一对句子为连续句子（即第二句是第一句的下一句），50%为随机选择的句子对。模型需要判别给定的两个句子是否为连贯的上下文。这种任务帮助模型学习文本之间的逻辑关系。

微调阶段
在微调阶段，预训练好的BERT模型会根据具体的下游任务进行调整。

描述一下 Transformer 的推理与训练过程
以翻译任务为例。

Transformer模型在处理文本翻译任务时，其推理过程可以分为几个关键步骤。以下是以“你好，世界”为例的详细推理过程：

输入编码：

首先，模型会将输入文本“你好，世界”转换为token序列。假设tokenizer将这句话分解为“你”、“好”、“，”、“世”、“界”。

每个token会被转换为一个embedding向量，这些向量包含了词汇的语义信息。

位置编码：

由于Transformer模型本身不包含循环结构，无法自然地处理序列的顺序信息，因此需要添加位置编码。位置编码会与token embedding相加，以提供序列中每个token的位置信息。

自注意力机制：

输入序列（包含位置编码的token embedding）会被送入多个自注意力层。在自注意力层中，模型会计算每个token与序列中其他token的关系，从而捕捉序列内部的依赖关系。

自注意力机制通过计算query、key、value矩阵来实现这一点，具体是通过点积计算注意力分数，然后对value进行加权求和。

前馈神经网络：

每个自注意力层的输出会被送入一个前馈神经网络，该网络对每个token进行非线性变换，进一步提取特征。

编码器堆叠：

上述的自注意力和前馈神经网络步骤会被重复多次（通常是6层或更多），形成编码器堆叠结构。每一层都会对输入序列进行更深层次的特征提取。

解码器操作：

在翻译任务中，解码器会接收编码器的输出，并生成目标语言的token序列。解码器同样包含自注意力和前馈神经网络层，但在自注意力层中，解码器还会考虑编码器的输出。

解码器在生成每个token时，会使用masked自注意力机制，确保每个token只能依赖于它之前的token，以保持自回归特性。

输出预测：

解码器生成目标语言的token序列后，模型会通过一个线性层和softmax函数来预测下一个token的概率分布。

在推理过程中，通常会使用贪婪搜索或束搜索等策略来选择最可能的token序列。

输出解码：

最终，模型会输出翻译结果，例如“Hello, World”。

整个推理过程是端到端的，从输入文本到输出翻译结果，Transformer模型通过其独特的自注意力和编码器-解码器结构，能够有效地捕捉长距离依赖关系，并生成高质量的翻译文本。

介绍一下 Embedding 技术
在NLP领域中Embedding即词嵌入技术，是一种将文本中的单词转换为固定长度的向量（词向量）技术。

为什么需要Embedding技术
机器学习需要数字化表达，因此文本数字化表达为向量才能被模型理解。

维度灾难。独热编码会导致特征向量极度稀疏。

词汇鸿沟。独热编码不能表达词汇之间的联系。

在Transformer中计算自注意力机制的时候，为什么除以 sqrt(d_k)，为什么不直接除以d_k，为什么不除以 d_k 的1/3次幂
为什么要进行缩放: 首先，进行缩放的主要目的是为了控制softmax函数的输入值的方差。如果不进行缩放，随着维度d_k的增加，点积的结果会变得很大，导致softmax函数的梯度变得很小，这会影响模型的学习。

为什么选择sqrt(d_k): 选择sqrt(d_k)是基于理论分析和经验的结果。假设查询(Q)和键(K)的各个分量是独立同分布的随机变量，均值为0，方差为1。那么它们的点积的方差将正比于d_k。通过除以sqrt(d_k)，我们可以将点积的方差缩放回1，保持一个稳定的分布。

为什么不直接除以d_k: 如果直接除以d_k，缩放可能会过度，使得点积的值变得过小。这可能会导致梯度消失问题，特别是在d_k较大的情况下。

为什么不除以d_k的1/3次幂: 虽然d_k的1/3次幂也是一种可能的选择，但它可能不如sqrt(d_k)有效。sqrt(d_k)提供了一个很好的平衡，既能控制方差，又不会过度压缩值的范围。

在Self Attention中，Q K V 一般会怎么初始化，如果使用全0初始化或者全1初始化可能会有什么问题
常见的初始化方法： a) 随机初始化：通常使用均匀分布或正态分布进行随机初始化。 b) Xavier/Glorot初始化：根据输入和输出的神经元数量来缩放初始化值。 c) He初始化：类似于Xavier初始化，但专门为ReLU激活函数设计。

全0初始化的问题： 如果使用全0初始化Q、K、V矩阵，可能会导致以下问题： a) 对称性问题：所有的注意力权重将会相同，导致模型无法学习到不同位置之间的差异。 b) 梯度消失：在反向传播时，由于所有权重都是0，梯度也会变为0，阻止模型学习。 c) 表达能力受限：模型将无法捕捉到输入序列中的任何模式或特征。

全1初始化的问题： 全1初始化同样会带来一些问题： a) 注意力分数过大：所有的注意力分数都会非常高，导致softmax函数的输出接近均匀分布。 b) 梯度爆炸：在反向传播时可能导致梯度爆炸，特别是在深层网络中。 c) 难以区分重要性：模型难以学习到哪些部分更重要，因为初始状态下所有位置都被同等对待。

为什么需要适当的初始化： 合适的初始化可以： a) 打破对称性，使得不同的神经元可以学习不同的特征。 b) 控制激活值和梯度的方差，防止梯度消失或爆炸。 c) 加速收敛，使模型更容易找到好的局部最优解。

Xavier 初始化
Xavier初始化的作者，Xavier Glorot，在Understanding the difficulty of training deep feedforward neural networks论文中提出一个洞见：激活值的方差是逐层递减的，这导致反向传播中的梯度也逐层递减。要解决梯度消失，就要避免激活值方差的衰减，最理想的情况是，每层的输出值（激活值）保持高斯分布。

因此，他提出了Xavier初始化：bias初始化为0，为Normalize后的参数乘以一个rescale系数：1 / \sqrt{n} ，n是输入参数的个数。

He 初始化
Xavier初始化的问题在于，它只适用于线性激活函数，但实际上，对于深层神经网络来说，线性激活函数是没有价值，神经网络需要非线性激活函数来构建复杂的非线性系统。今天的神经网络普遍使用relu激活函数。

因为relu会抛弃掉小于0的值，对于一个均值为0的data来说，这就相当于砍掉了一半的值，这样一来，均值就会变大，根据新公式的推导，最终得到新的rescale系数：\sqrt{2 / n} 。

如何避免模型的过拟合，如果训练数据量足够大，就一定不会过拟合吗
1. 增加数据量
获取更多数据：更多的数据可以帮助模型更好地学习，减少对特定训练数据的依赖。

数据增强：通过随机裁剪、旋转、翻转、添加噪声等方式生成更多的训练样本，特别是在图像和文本处理领域。

2. 正则化
L1/L2正则化：向损失函数中添加权重惩罚项，L1正则化会导致稀疏解，L2正则化则使权重减小，防止模型过于复杂。

Dropout：在每次训练时，随机丢弃一部分神经元，防止神经网络对某些特定路径过度依赖。

Early Stopping（提前停止）：在验证集的损失不再降低时停止训练，避免模型继续过拟合训练数据。

3. 简化模型
减少模型复杂度：通过降低模型的参数数量，减少神经网络的层数或每层的神经元数量来降低模型的复杂度。

特征选择：移除不相关或噪声较大的特征，降低模型的复杂性。

4. 交叉验证
K折交叉验证：将数据集分为K份，轮流将其中一份作为验证集，剩余的作为训练集。通过多次训练与验证，减少模型对特定数据的依赖，提高模型的泛化能力。

5. 集成学习
Bagging：通过训练多个独立的模型并将它们的结果进行平均或投票，可以减少单一模型的过拟合风险。随机森林就是一个常见的例子。

Boosting：通过训练一系列弱学习器，并逐步增强难以预测的样本，提升整体模型的性能，同时也可以减少过拟合。

6. 数据预处理
归一化/标准化：对数据进行归一化或标准化，使特征值在同一尺度上，有助于模型更稳定的收敛。

移除噪声：清理数据中的异常值或噪声，减少不必要的复杂度。

L1 正则化与 L2 正则化
L1正则化（Lasso Regularization）
L1正则化是在模型的损失函数中添加权重的绝对值之和作为惩罚项。

特点 ：

稀疏性 ：L1正则化倾向于产生稀疏的权重矩阵，即很多权重参数会被强制设置为0。这种特性使得L1正则化可以用作特征选择，因为它能够自动地忽略那些不重要的特征。

对异常值不敏感 ：由于惩罚的是权重的绝对值，因此L1正则化对于异常值有一定的鲁棒性。

L2正则化（Ridge Regularization）
L2正则化是在模型的损失函数中添加权重的平方和作为惩罚项。常见的是使用权重向量的L2范数的平方。

特点 ：

权重衰减 ：L2正则化通过惩罚权重的平方，使得权重值趋向于减小，但不会变成0。这有助于防止模型过拟合，因为它减少了模型对单个权重的高度依赖。

稳定性 ：相比于L1正则化，L2正则化通常会导致更稳定的权重更新，因为它不会导致权重参数的急剧变化。

当特征数量很多，且希望自动进行特征选择时，L1正则化是一个好的选择。而当目标是减少过拟合，但不希望减少特征数量时，L2正则化更为合适。

Dropout 是什么，有什么用
Dropout是一种用于深度学习网络中的正则化技术，旨在防止模型过拟合。它由Hinton等人在2012年提出，并在神经网络训练中得到了广泛应用。

Dropout的工作原理：
在训练过程中，Dropout会随机“丢弃”（即设置为零）网络中的一部分神经元，包括它的权重和激活。具体来说，对于每个训练批次，每个神经元都有一定的概率 p（称为dropout rate）被暂时从网络中移除。在剩余的神经元上继续进行前向传播和反向传播，但权重更新时会考虑到被丢弃的神经元。

Dropout的作用：
防止过拟合 ：通过随机丢弃神经元，Dropout减少了网络对特定神经元的依赖，迫使网络学习更加鲁棒的特征表示，这有助于提高模型在未见数据上的泛化能力。

实现模型集成 ：Dropout可以看作是一种廉价的模型集成方法。每次迭代时，由于不同的神经元被丢弃，实际上是在训练不同的网络结构。在测试时，所有这些网络的预测可以以某种方式（如取平均值）结合起来，从而提高模型的性能。

减少参数数量 ：虽然Dropout本身并不直接减少模型参数的数量，但由于它在训练过程中减少了活跃神经元的数量，可以在一定程度上减少模型对参数的依赖。

Dropout的使用：
设置dropout rate：通常，dropout rate设置在0.2到0.5之间，这个值需要根据具体问题和网络结构进行调整。

训练与测试 ：在训练时应用dropout，而在测试时则不应用。为了在测试时保持输出的一致性，通常需要在训练时将每个神经元的输出乘以1−p，这样在测试时就不需要任何修改，因为所有神经元的输出都被“保留”。

Dropout的注意事项：
批量大小 ：在使用Dropout时，建议使用较大的批量大小，因为这样可以减少随机性的影响，使梯度估计更加稳定。

学习率 ：在使用Dropout时，可能需要调整学习率，因为网络的有效容量会因为Dropout而降低。

与其他正则化技术结合 ：Dropout可以与其他正则化技术（如L1、L2正则化）结合使用，以达到更好的正则化效果。

常用的优化器有哪些，Adam的有哪些优点
1. 梯度下降法（Gradient Descent）
批量梯度下降（Batch Gradient Descent）

概述：在每次迭代中，使用整个训练集计算梯度并更新参数。

优点：收敛到全局最优解，更新方向稳定。

缺点：对内存要求高，计算开销大，尤其是当训练集很大时。

随机梯度下降（Stochastic Gradient Descent, SGD）

概述：在每次迭代中，只使用一个样本计算梯度并更新参数。

优点：计算效率高，对内存要求低，可以跳出局部最优解。

缺点：更新方向不稳定，容易受到噪声影响，收敛较慢。

小批量梯度下降（Mini-batch Gradient Descent）

概述：在每次迭代中，使用一个小批量的样本（如32或64个样本）计算梯度并更新参数。

优点：在计算效率和稳定性之间取得平衡，比SGD收敛更快且更稳定。

缺点：可能仍会陷入局部最优解。

2. 动量法（Momentum，SGDM）
概述：在每次更新时，结合当前梯度和上一次更新方向的加权平均，类似于物理中的动量概念。

优点：可以加速收敛，尤其是在长而平坦的谷底；有助于跳出局部最优解。

缺点：需要选择合适的动量参数（通常是0.9），否则可能导致振荡。

3. Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）
概述：在动量法的基础上，首先利用动量估计未来的位置，然后在此位置上计算梯度。

优点：比普通动量法收敛更快，更早地纠正了更新方向，避免过冲。

缺点：增加了计算复杂性。

4. AdaGrad（Adaptive Gradient）
概述：为每个参数维护一个自适应的学习率，学习率根据参数历史梯度的平方和的倒数进行调整。通过对每个参数的历史梯度平方和进行累加，从而调整每个参数的学习率。频繁更新的参数会得到较小的学习率，而不常更新的参数会得到较大的学习率。

优点：适合处理稀疏数据，能够自动调整学习率。

缺点：学习率随着时间推移不断减小，可能导致过早停止学习。

5. RMSProp（Root Mean Square Propagation，根均方传播）
概述：对 AdaGrad 进行改进，使用指数加权移动平均数来累积平方梯度，防止学习率过早减小。

优点：适用于非平稳目标（如深度学习中的损失函数），收敛更快且更稳定。

缺点：引入了超参数（通常是0.9），需要调优。

6. Adam（Adaptive Moment Estimation，自适应矩估计）
概述：Adam结合了动量法和 RMSProp 的思想，同时利用梯度的一阶矩（动量）和二阶矩（平方梯度）的指数移动平均值来调整学习率。

在统计学中，一阶矩通常指的是随机变量的期望值，即平均值。在优化算法中，一阶矩指的是梯度的指数移动平均值，它可以看作是梯度的“动量”部分。在Adam算法中，一阶矩用于累积梯度的历史信息，帮助算法在梯度方向一致的维度上加速更新。

二阶矩在统计学中通常指的是随机变量的方差。在优化算法中，二阶矩指的是梯度平方的指数移动平均值，它可以看作是梯度的“均方根”部分。在Adam算法中，二阶矩用于自适应地调整每个参数的学习率，使得在梯度较大的维度上减小学习率，在梯度较小的维度上增大学习率。

优点：广泛应用于深度学习，能够快速收敛，自适应调整学习率，对初始学习率敏感度较低。

缺点：可能需要进行超参数调优（如学习率、动量参数等），在某些情况下可能不如SGD泛化能力好。

了解过 Boosting 系列算法吗
Boosting系列的算法是一组集成学习算法，它们通过顺序地训练一系列弱学习器（通常是决策树），并将它们组合成一个强学习器，以此来提高整体的预测性能。Boosting算法的核心思想是利用一系列的弱学习器，通过加权投票的方式，形成一个性能更好的强学习器。

以下是Boosting系列算法的一些关键特点：

顺序性 ：Boosting算法是顺序训练的，每个新的学习器都尝试纠正前一个学习器的错误。

权重调整 ：在训练过程中，每个样本的权重会根据前一个学习器的表现进行调整。被错误分类的样本权重会增加，而被正确分类的样本权重会减少，这样后续的学习器会更加关注难分的样本。

弱学习器 ：Boosting算法通常使用简单的模型作为基学习器，这些模型单独表现一般，但组合起来可以非常强大。

加权组合 ：每个学习器在最终的预测中都有一定的权重，这些权重是基于学习器在训练过程中的表现来确定的。

以下是几种流行的Boosting算法：

AdaBoost（Adaptive Boosting） ：是最著名的Boosting算法之一，它通过调整样本权重来训练一系列的分类器，并将这些分类器加权组合起来。

Gradient Boosting：是一种通用的框架，它可以用于分类和回归问题。它通过最小化损失函数来训练一系列的决策树，每棵树都是在前一棵树的基础上构建的。

XGBoost：是一个优化的分布式Gradient Boosting库，它提供了快速的运算和更好的模型性能。XGBoost在系统优化和算法实现上做了很多改进。

LightGBM：是另一个高效的Gradient Boosting框架，它使用了基于直方图的算法和GOSS（Gradient-based One-Side Sampling）等技术来提高训练速度和减少内存使用。

CatBoost：是一个基于对称决策树的Gradient Boosting算法，它特别擅长处理类别特征，并且减少了过拟合的风险。

Boosting算法在许多机器学习竞赛和实际应用中表现出色，尤其是在结构化数据上。然而，它们也可能导致过拟合，特别是当模型非常复杂或者训练数据量不足时。因此，在使用Boosting算法时，需要进行适当的正则化，并且仔细调整超参数。

区别：
样本权重更新 ：AdaBoost通过改变样本权重来影响后续的分类器，而Gradient Boosting、XGBoost和LightGBM则是通过拟合残差来构建模型。

损失函数 ：AdaBoost通常用于分类问题，而Gradient Boosting、XGBoost和LightGBM可以用于回归和分类问题，并且可以自定义损失函数。

优化和效率 ：XGBoost和LightGBM在系统优化方面做了很多工作，使得它们在处理大规模数据时更加高效。

正则化 ：XGBoost和LightGBM在模型训练中加入了更多的正则化项，有助于防止过拟合。

Llama 中的旋转位置编码和绝对位置编码有什么区别？如果序列很长，用什么编码方式比较适合？
Llama模型中的旋转位置编码（RoPE，Rotation Position Encoding）和绝对位置编码（Absolute Position Encoding）都是用来在Transformer模型中引入位置信息的方法。它们的主要区别在于编码方式和效果上：

旋转位置编码（RoPE） ：

原理 ：RoPE通过将每个位置的编码与一个旋转矩阵相乘来引入位置信息。这种方法允许模型捕获位置之间的相对关系，因为旋转矩阵的乘法本质上是一种相对位置的操作。

优点 ：可以更好地处理长序列，因为它是相对位置的，不受序列长度的影响。

适用性 ：对于非常长的序列，RoPE通常更为有效，因为它能够保持相对位置信息的准确性。

绝对位置编码 ：

原理 ：绝对位置编码为序列中的每个位置分配一个唯一的编码向量。这些向量通常是固定的或者通过学习得到的，直接加到输入嵌入上。

优点 ：实现简单，对于不太长的序列，能够有效引入位置信息。

缺点 ：当序列长度增加时，绝对位置编码可能无法很好地处理超出训练时最长序列长度的位置信息。

什么是奥卡姆剃刀原则
奥卡姆剃刀原则（Occam's Razor）是一个解决问题的原则，它指出在解释任何现象时，应当尽可能少地做出假设。其核心理念是“如无必要，勿增实体”，也就是说，在没有充分理由的情况下，不应该引入更多复杂的概念或假设。

简单来说，奥卡姆剃刀原则鼓励我们选择最简单、最少假设的解释作为最佳解释。这个原则并不一定意味着最简单的解释就是正确的，但它提供了一个优先考虑简单性的标准，因为在没有额外证据支持更复杂解释的情况下，简单的解释往往更可靠。

在科学研究和理论构建中，奥卡姆剃刀原则经常被用作一个启发式的工具，帮助科学家们筛选和评估不同的理论。它提醒研究者避免过度拟合理论，即避免构建过于复杂、包含许多不必要的假设的理论。

然而，奥卡姆剃刀原则并不是一个决定性的原则，而是一个指导性的原则。有时，更复杂的理论可能因为提供了更好的预测或者更全面地解释了现象而被接受。在这种情况下，额外的复杂性是有充分理由的，因此符合奥卡姆剃刀原则的精神。

经验风险是什么
经验风险（Empirical Risk）是机器学习中用来评估模型性能的一个概念。它指的是模型在训练数据集上的平均损失。具体来说，经验风险是通过对训练数据集中的每个样本应用损失函数（例如平方损失、交叉熵损失等）来计算的，然后取所有样本损失的平均值。

经验风险是模型训练过程中的一个关键指标，因为它直接关联到模型在训练数据上的表现。最小化经验风险是训练模型的一个主要目标，这通常通过优化算法（如梯度下降）来实现。

然而，经验风险最小化并不总是能保证模型在未见过的数据（即测试数据）上表现良好。这是因为模型可能会过拟合训练数据，即模型学习到了训练数据中的噪声和特异性，而没有捕捉到数据的真实分布。因此，除了经验风险，机器学习中还会考虑其他指标，如泛化误差（Generalization Error），来评估模型的泛化能力。

预训练模型的输出维度和下游任务所需的维度不匹配，应该怎么解决
当预训练模型的输出维度与下游任务所需的维度不匹配时，有几种方法可以处理这种维度不一致的问题：

增加全连接层（Dense Layer） ：

在预训练模型之后添加一个或多个全连接层，最后一个全连接层的输出维度设置为1024。这样可以有效地将512维的输出转换为1024维。

例如，可以添加一个512到1024的线性层，并可选地在这之前添加一些非线性激活层（如ReLU）。

维度扩展 ：

使用某种形式的维度扩展技术，如重复（Repetition）、复制（Duplication）或插值（Interpolation），将512维的数据扩展到1024维。

例如，可以将512维向量复制一次，然后连接起来形成1024维向量。

特征映射 ：

设计一个映射函数，将512维的特征映射到1024维。这可以通过学习一个矩阵来完成，该矩阵将512维输入转换为1024维输出。

特征融合 ：

如果下游任务有额外的输入或特征，可以考虑将这些特征与预训练模型的输出进行融合，以达到所需的1024维。

在实际操作中，通常会选择增加全连接层的方法，因为它是最灵活且在神经网络中常见的方法。这种方法可以通过训练过程中的反向传播自动学习如何最好地将512维的特征转换为1024维。其他方法可能需要更复杂的定制或额外的假设。无论选择哪种方法，都应该在下游任务的验证集上评估其效果，以确保模型的性能不受影响。

讲一下AUC，最好的AUC曲线应该是什么样子
AUC（Area Under Curve），即曲线下面积，是评估分类模型性能的重要指标，特别是在处理不平衡数据集时非常有用。AUC通常与ROC（Receiver Operating Characteristic）曲线一起使用。

ROC曲线
ROC曲线是一种图形工具，用于评价二分类模型的性能。它通过显示真阳性率（True Positive Rate，TPR）与假阳性率（False Positive Rate，FPR）之间的权衡来帮助我们理解模型的分类能力。在二分类问题中，模型会预测每个样本属于正类的概率，然后根据这些概率来绘制ROC曲线。

AUC
AUC是ROC曲线下的面积，其值域为0,10,1。AUC值提供了衡量分类模型整体表现的定量指标：

AUC=1：表示完美分类器，所有正例都排在负例前面。

0.5<AUC<1：优于随机猜测，但仍有提升空间。

AUC=0.5：与随机猜测相同，模型没有预测价值。

AUC<0.5：比随机猜测还差，这种情况很少见。

最好的AUC曲线
理想的AUC曲线应该是紧贴着左上角，即ROC曲线尽可能地接近于坐标轴的左上角。这意味着模型能够非常有效地将正样本排在负样本前面，从而具有很高的分类性能。

ROC和AUC的优势
不依赖于类别分布 ：ROC曲线和AUC值不受正负样本比例的影响，因此在处理不平衡数据集时特别有用。

直观展示模型性能 ：ROC曲线可以直观地展示模型在不同阈值下的性能。

易于比较不同模型 ：通过比较不同模型的ROC曲线和AUC值，可以快速评估和比较模型的性能。

综上所述，AUC和ROC曲线是评估和优化分类模型性能的重要工具，特别是在处理不平衡数据集时。

BERT 和 RoBERTa 有什么区别
BERT（Bidirectional Encoder Representations from Transformers）和RoBERTa（Robustly optimized BERT approach）都是基于Transformer架构的预训练语言模型，它们在很多自然语言处理（NLP）任务中都取得了显著的效果。以下是BERT和RoBERTa之间的一些主要区别：

预训练任务 ：

BERT：使用了Masked Language Model（MLM）和Next Sentence Prediction（NSP）两个预训练任务。MLM任务涉及随机掩盖输入文本中的某些词，并要求模型预测这些词；NSP任务则是预测两个句子是否是连续的。

RoBERTa：去掉了NSP任务，只使用了MLM任务。RoBERTa的作者发现NSP任务对于模型性能的提升并不显著，而且可能会引入噪声。

训练数据量 ：

BERT：使用了英文维基百科（约2500万篇文章，1100亿个词）和书籍语料库BookCorpus（约1300万个词）进行预训练。

RoBERTa：使用了更大的数据集，包括英文维基百科的全部内容（约3800万篇文章，1350亿个词）和额外的CommonCrawl数据（约60亿个网页）。

动态掩码 ：

BERT：在预训练数据准备阶段，掩码策略是静态的，即一旦掩码，在整个训练过程中保持不变。

RoBERTa：引入了动态掩码策略，即在每个训练epoch中都会随机生成不同的掩码，这样可以让模型学习到更加鲁棒的语言表示。

训练批次大小和步数 ：

BERT：使用了较小的批次大小和较少的训练步数。

RoBERTa：使用了更大的批次大小和更多的训练步数，这有助于模型在预训练过程中捕捉到更多的语言特征。

文本编码 ：

BERT：在处理输入文本时，使用了WordPiece分词器，并将文本分割成更小的token。

RoBERTa：同样使用WordPiece分词器，但是对BERT的文本编码策略进行了一些改进，例如，不再使用句子A和句子B的区分标记，而是使用更多的分隔符来处理长文本。

模型变种 ：

BERT：发布了多个不同大小的模型，如BERT-Base和BERT-Large。

RoBERTa：也发布了不同大小的模型，并且在某些任务上性能优于同等大小的BERT模型。

总的来说，RoBERTa可以看作是BERT的一个改进版本，它在预训练数据量、训练策略和模型细节上进行了优化，从而在很多NLP任务上取得了更好的性能。

讲述一下Transformer模型中注意力机制的演进
MHA（Multi-Head Attention），也就是多头注意力，是开山之作《Attention is all you need》所提出的一种 Attention 形式，可以说它是当前主流 LLM 的基础工作。在数学上，多头注意力 MHA 等价于多个独立的单头注意力的拼接。

而后面的 MQA、GQA、MLA，都是围绕“如何减少 KV Cache 同时尽可能地保证效果”这个主题发展而来的产物。

KV Cache
LLM推理的过程是一个自回归的过程，也就是说前i次的token会作为第i+1次的预测数据送入模型，拿到第i+1次的推理token。在这个过程中Transformer会执行自注意力操作，为此需要给当前序列中的每个项目（无论是prompt/context还是生成的token）提取键值（kv）向量。这些向量存储在一个矩阵中，通常被称为kv cache。kv cache是为了避免每次采样token时重新计算键值向量。利用预先计算好的k值和v值，可以节省大量计算时间，尽管这会占用一定的存储空间。

Page Attention
PageAttention命名的灵感来自OS系统中虚拟内存和分页的思想。可以实现在不连续的空间存储连续的kv键值。

所有键值都是分布存储的，需要通过分页管理彼此的关系。序列的连续逻辑块通过 block table 映射到非连续物理块。

同一个prompt生成多个输出序列，可以共享计算过程中的attention键值，实现copy-on-write机制，即只有需要修改的时候才会复制，从而大大降低显存占用。

MHA/GQA/MQA 优化技术
img

MQA (Multi-Query Attention)
概述：MQA提出了一种混合的方式来处理查询、键和值，从而在减少计算量的同时，保持模型的表现。

工作原理：

MQA通过减少查询头的数量（即减少Q的投影次数），但允许每个查询头仍然和多个键和值进行交互。也就是说，多个注意力头共享查询子空间的表示。

这种机制在降低计算复杂度的同时，仍然可以捕捉到多个子空间的信息。

使用 MQA 的模型包括 PaLM [6]、StarCoder [7]、Gemini [8] 等。

GQA (Grouped-Query Attention)
GQA通过减少注意力头的数量，但保持注意力头之间的多样性，来降低计算复杂度。

工作原理：

在GQA中，多个查询头共享一个键和值的子空间。这意味着不同的查询头关注的是同一组键和值，而不是像MHA那样完全独立。

通过这种共享机制，减少了独立投影所需的计算量，同时仍然保留了多样性。

GQA 提供了 MHA 到 MQA 的自然过渡。Meta 开源的 LLAMA2-70B [10]，以及 LLAMA3 [11] 全系列，ChatGLM系列使用的是GQA。

MLA (Multi-head Latent Attention)
似乎是在DeepSeek-v2中用到的

在标准的MHA结构中，每个token的query、key和value通过参数矩阵映射得到，并分割成多个注意力头。每个头独立计算注意力权重并得到输出，这个过程虽然能捕捉丰富的上下文信息，但在推理时需要缓存大量的KV Cache。

为每个注意力头使用独特的投影矩阵来增强GQA的能力，同时仍与MHA共享相同的键-值（KV）缓存大小。（MLA通过对keys和values进行低秩联合压缩来降低KV Cache）

MLA 怎么与 RoPE (旋转位置编码) 做结合
接下来 MLA 讨论的一个问题是，在上面的压缩过程中我们并没有考虑到 RoPE。原始的 RoPE 需要在 query 和 key 中融入相对位置信息。在 MLA 中，在 query 中融入相对位置信息是比较容易的，但是由于 KV Cache 缓存的是压缩后的低秩 key-value 信息，这里面是没办法融入相对位置信息的。这就意味着，推理时我们必须重新计算所有之前 tokens 的 keys，这将大大降低推理效率。

image-20240819180233391

Flamingo 的具体实现
Flamingo模型的核心思想是将预训练好的视觉特征编码器（如ViT、Resnet等）和大语言模型进行结合。视觉特征编码器负责将图像信息转换成特征向量，而大语言模型则负责将这些特征向量与文本信息进行交织和融合。通过这种方式，Flamingo模型可以实现对图像和文本信息的联合建模，从而在处理多模态信息时具有更强的泛化能力和鲁棒性。

在具体实现上，Flamingo模型采用了感知重采样技术和门控交叉注意力技术。Flamingo的视觉编码器和LLM都是固定参数，不在训练中更新。感知重采样器将变长的视觉向量转换成定长的多模态语义向量，而门控交叉注意力技术则负责将这些多模态语义向量与文本信息进行融合。通过这种方式，Flamingo模型可以在输入中混合多模态信息，并输出相应的文本信息。

其中的视觉编码器采用NFNet（NormalizerFree ResNet），作者先在图文对数据上采用CLIP的方式对NFNet进行预训练，随后进行参数固定。如果视觉端输入是视频，则按照1 fps进行采样后将N NN帧进行视觉特征提取（若是图片输入，则N=1），注意到此时position embedding按照帧粒度组织，即是统一帧的不同patch共用一个position embedding以建模帧间序列信息。尔后对多帧的特征进行展开、拼接，作为transformer的k和v，而采用一个可学习的query向量作为transformer的q输入，感知重采样机制的一个好处就是，可以将变长的视频输入转变为定长的输入，此处定长的输入长度为64。

门控注意力单元的设计，则是在原先固定的LLM结构的每一层基础上叠加了门控单元，门控单元由交叉注意力机制和门控结构、FFW交替组成，其中交叉注意力的k和v都是感知重采样器的输出，而q则是文本输入。为了保证在训练初始阶段模型和原先的LLM不至于偏差太远，作者采用了门控机制。

Pre-Norm 和 Post-Norm 有什么区别
Bert的Post-Norm，是在Add操作后进行Norm操作，因此叫做Post-Norm。而Pre-Norm则是Norm之后再Add，所以叫Pre-Norm。

简单说就是Post-Norm由于是在残差之后进行归一化，因此归一化的效果更好，使得模型的鲁棒性更强。

而Pre-Norm由于并不是所有的参数都参与正则化，因此整体来说更不容易发生梯度消失的问题，模型训练的稳定性更强。

因此，在Bert时代由于层数较浅，往往采用的是Post-Norm，而到了大模型时代，由于transformer的层数开始加深，为了训练稳定性开始使用Pre-Norm。

扩展：DeepNorm
作者认为，出现训练不稳定（梯度消失/爆炸）的主要原因是同时更新的参数过多（爆炸式更新）。

有一种技巧是 warmUp 即刚开始使用更小的学习率进行训练，然后慢慢增大训练

Post-LN-no warmup在一开始更新了过多的参数，导致模型错误的进入了一个局部最优解。由于LN的梯度大小与LN的输入成反比，因此当更新的参数量过大时，会使LN的输入变大，因此这也导致了LN梯度逐渐消失，使得模型很难脱离出局部最优解，进一步破坏了算法的稳定性。而Post LN init或者warmup更新的参数很小，LN相对稳定，减轻了梯度消失的可能性，使得训练更加稳定。 

DeepNorm主要包括两部分，一个是模型初始化，另一个是对Post-Norm的改进。研究者在残差连接处引入了一个新的归一化函数 —— DEEPNORM，它从理论上保证了把模型更新过程限制为常数。

BERT 有多少层
BERT-base 版本有 12 层（transformer 层），每层有 768 个隐藏单元，模型参数总数约为 1.1 亿。

BERT-large 版本有 24 层，每层有 1024 个隐藏单元，模型参数总数约为 3.4 亿。

位置编码相关问题
Transformer 中为什么要有位置编码
在Transformer模型中，使用位置编码（Position Encoding）是为了提供序列中单词的位置信息，因为Transformer架构本身是完全基于自注意力机制的，没有内置的顺序信息。具体来说，位置编码的必要性可以归纳为以下几点：

无序性 ：传统的循环神经网络（RNN）和卷积神经网络（CNN）能够通过其结构自然地处理序列数据的顺序。RNN通过时间步的顺序处理输入，而CNN通过局部感受野捕捉上下文信息。而Transformer使用自注意力机制，使得输入序列中的每个元素可以在任何位置直接与其他元素进行交互，这种机制本质上是无序的。

捕捉位置信息 ：为了让模型理解单词在句子中的相对位置，位置编码被添加到输入的词向量中。这样，模型就能够区分“我爱你”和“你爱我”这样的句子，因为它们的单词顺序不同。

相对位置 ：位置编码通常是通过正弦和余弦函数生成的，这样可以产生不同频率的编码，使得模型能够捕捉到不同位置之间的相对距离。这种设计使得模型在处理长序列时能够更好地理解相对位置。

平滑性 ：通过使用连续的正弦和余弦函数，位置编码在不同位置之间提供了一种平滑的过渡，这有助于模型的学习过程。

BERT的位置编码和Transformer的位置编码有什么区别
Transformer：原始的Transformer模型（如Vaswani等人在2017年提出的模型）使用正弦和余弦函数 进行位置编码。这种编码是固定的，且不依赖于输入的数据内容。具体来说，位置编码是基于序列位置生成的，通过将每个位置映射为不同频率的正弦和余弦值。这种编码方式使得模型能够感知序列中不同位置的信息。

BERT：BERT采用的是可训练的嵌入（embedding） 作为位置编码。这意味着，位置编码不是基于固定的函数生成，而是通过训练过程中的反向传播进行调整和优化。这样，BERT可以学习到更加适应特定任务的位置信息编码。

ROPE 位置编码：为什么它比绝对或相对位置编码更好?
旋转位置嵌入是最先进的 NLP 位置嵌入技术。大多数流行的大型语言模型（如 Llama、Llama2、PaLM 和 CodeGen）已经在使用它。

相对位置编码有一些问题：

计算效率低下：必须创建成对的位置编码矩阵，然后执行大量张量操作以获得每个时间步的相对位置编码。特别是对于较长的序列。这主要是由于自注意力层中的额外计算步骤，其中位置矩阵被添加到查询键矩阵中。

键值缓存使用的复杂性：由于每个附加令牌都会改变每个其他令牌的嵌入，这使得 Transformer 中键值缓存的有效使用变得复杂。使用 KV 缓存的一项要求是已经生成的单词的位置编码， 在生成新单词时不改变（绝对位置编码提供）因此相对位置编码不适合推理，因为每个标记的嵌入会随着每个新时间步的变化而变化。

RoPE  代表了一种编码位置信息的新方法。传统方法中无论是绝对方法还是相对方法，都有其局限性。绝对位置编码为每个位置分配一个唯一的向量，虽然简单但不能很好地扩展并且无法有效捕获相对位置；相对位置编码关注标记之间的距离，增强模型对标记关系的理解，但使模型架构复杂化。

RoPE巧妙地结合了两者的优点。允许模型理解标记的绝对位置及其相对距离的方式对位置信息进行编码。这是通过旋转机制实现的，其中序列中的每个位置都由嵌入空间中的旋转表示。RoPE 的优雅之处在于其简单性和高效性，这使得模型能够更好地掌握语言语法和语义的细微差别。

旋转矩阵的原理

img

RoPE 引入了一个新颖的概念。它不是添加位置向量，而是对词向量应用旋转。旋转角度 (θ) 与单词在句子中的位置成正比。第一个位置的向量旋转 θ，第二个位置的向量旋转 2θ，依此类推。这种方法有几个好处：

向量的稳定性：在句子末尾添加标记不会影响开头单词的向量，有利于高效缓存。

相对位置的保留：如果两个单词在不同的上下文中保持相同的相对距离，则它们的向量将旋转相同的量。这确保了角度以及这些向量之间的点积保持恒定

img

Transformer中为什么要scale？为什么scale的数值是 sqrt(dk)  
均值为 0 ⽅差为 1 的独⽴随机变量的内积⽅差为维度⻓度 dk ，所以除标准差 sqrt(dk) ◦ 

如果不scale，随着层数的加深，这个⽅差会越来越⼤，最终出现极⼤值，使softmax梯度消失 （softmax输⼊的数字量级越⼤，导数也越接近0） ◦ 

QK的乘积太⼤，softmax之后每⼀个位置的梯度太⼩了，所以需要除⼀个scale值来放缩

Transformer 为什么要使用 layer norm 不使用 batch norm
在Transformer模型中，使用Layer Normalization（层归一化）而不是Batch Normalization（批归一化）有几个关键原因：

序列长度的变化 ：在自然语言处理任务中，输入序列的长度可能会有很大变化。Batch Normalization依赖于批量中的样本数量，如果批量大小不一致，会导致归一化效果不稳定。而Layer Normalization是对每个样本独立进行归一化，不受序列长度变化的影响。

小批量问题 ：在某些情况下，批量大小可能很小，这会导致Batch Normalization的统计特性不准确。Layer Normalization不依赖于批量大小，因此在小批量情况下表现更稳定。

训练和推理的一致性 ：Batch Normalization在训练和推理时使用不同的统计特性（训练时使用批量统计，推理时使用全局统计），这可能导致训练和推理之间的不一致性。Layer Normalization在训练和推理时使用相同的归一化方式，保持了一致性。

并行化能力 ：Layer Normalization可以更容易地在不同的序列上并行化，因为它不依赖于批量中的其他样本。这对于Transformer模型中的自注意力机制尤为重要，因为自注意力机制本身就是高度并行的。

对初始化的敏感性 ：Batch Normalization对权重初始化较为敏感，而Layer Normalization在这方面更为稳健。

综上所述，Layer Normalization在Transformer模型中提供了更好的稳定性、一致性和并行化能力，使其成为更合适的选择。

Pre-Norm 和 Post-Norm 有什么区别
原版Transformer和BERT使用了 Post Normalization
为什么使用 Post Normalization ：残差会进一步放大方差，所以我们也要想相应的策略缩小其方差。
一个朴素的想法是在残差后面加一个 Layer Normalization 。但是这种方法会削弱 残差的恒等分支 ，即削弱本来流向 x 的梯度，从而弱化了残差 易于训练 的优点。

Post-Normalization（后归一化）在训练深度学习模型时可能更加难以训练，主要是因为它的归一化过程是在每个层的输出之后进行的，这可能导致以下几个问题：

梯度消失或爆炸 ：后归一化可能会使得梯度在反向传播时更容易消失或爆炸，尤其是在深层网络中。这是因为在每一层的输出之后进行归一化，可能会影响到梯度的传播，使得模型的训练变得更加不稳定。

学习率敏感性 ：后归一化使得模型对学习率的选择更加敏感。如果学习率设置得不合适，模型可能会在训练初期表现得不稳定，导致收敛困难。

训练初期的不稳定性 ：在训练的初期，模型的参数可能还没有得到良好的初始化，导致输出的分布不稳定。此时进行归一化可能会导致输出值的剧烈波动，从而影响训练过程。

为了应对这些问题，通常会采用“warm-up”策略，即在训练的初期使用较小的学习率，逐渐增加到预设的学习率。这种策略可以帮助模型在训练初期稳定下来，避免因过大的学习率导致的训练不稳定，并且有助于模型逐渐适应后归一化带来的影响。

Self Attention 的时间复杂度计算
 在这里插入图片描述

image-20240826104640332

大模型精度问题（FP16，FP32，BF16）
FP16
FP16也叫做 float16，两种叫法是完全一样的，全称是Half-precision floating-point(半精度浮点数)，在IEEE 754标准中是叫做binary16，简单来说是用16位二进制来表示的浮点数

img

Sign(符号位): 1 位，0表示整数；1表示负数。

Exponent(指数位)：5位，简单地来说就是表示整数部分，范围为00001(1)到11110(30)，正常来说整数范围就是 21−230 ，但其实为了指数位能够表示负数，引入了一个偏置值，偏置值是一个固定的数，它被加到实际的指数上，在二进制16位浮点数中，偏置值是 15。这个偏置值确保了指数位可以表示从-14到+15的范围即 2−14−215 ，而不是1到30，注：当指数位都为00000和11111时，它表示的是一种特殊情况，在IEEE 754标准中叫做非规范化情况，后面可以看到这种特殊情况怎么表示的。

Fraction(尾数位)：10位，简单地来说就是表示小数部分，存储的尾数位数为10位，但其隐含了首位的1，实际的尾数精度为11位，这里的隐含位可能有点难以理解，简单通俗来说，假设尾数部分为1001000000，为默认在其前面加一个1，最后变成1.1001000000，换成10进制有两种方式

# 第一种计算方式
1.1001000000 = 1 * 2^0 + 1 * 2^(-1) + 0 * 2^(-2) + 0 * 2^(-3) + 1 * 2^(-4) + 0 * 2^(-5) + 0 * 2^(-6) + 0 * 2^(-7) + 0 * 2^(-8) + 0 * 2^(-9) = 1.5625
# 第二种计算方式
1.1001000000 = 1 + 576(1001000000变成10进制)/1024 = 1.5625
FP16(float16)表示的范围[-65504，65504]。

img

Normal Number（规范数）
规范数是浮点数表示中一种标准形式。它的特点是数值的有效位（即尾数部分）在规定的范围内，通常是以 1.xxxx 的形式表示（在二进制中）。在 IEEE 754 标准中，规范数的指数部分是经过偏移的

Subnormal Number（非规范数）
非规范数（或称为次规范数）是浮点数的一种特殊情况，其有效位不以 1 开头，而是以 0 开头。

接下来看一下在pytorch中是如何表示的：

torch.finfo(torch.float16)
finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)
eps

定义 ：eps 是指浮点数表示的最小正数，它与 1.0 的差值是可区分的最小值。也就是说，eps 是 FP16 中可以表示的最小的正数，使得 1.0 和 (1.0 + eps) 之间的差值可被识别。

用途 ：用于数值稳定性和避免除以零等问题。

resolution

定义 ：resolution 是指在 FP16 表示中最小的可区分的增量。它通常等同于 eps，表示 FP16 能够分辨的最小数值变化。

用途 ：用于确定浮点数计算的精度，尤其是在数值计算的上下文中。

tiny

定义 ：tiny 是指 FP16 中的最小非零值，通常是次规范数（subnormal number）中的最小值。它表示 FP16 能够表示的最小的非零数，但不是规范数。

用途 ：用于表示非常小的数值，尤其在需要很小的数值时，例如在某些数值算法中。

smallest_normal

定义 ：smallest_normal 是指 FP16 中的最小规范数（normal number）。这是 FP16 能够表示的最小的规范数。

用途 ：用于确定 FP16 能够表示的最小的有效数值，通常用于浮点数运算的范围界限。

BP16
BF16也叫做bfloat16(这是最常叫法)，其实叫“BF16”不知道是否准确，全称brain floating point。和上述FP16不一样的地方就是指数位和尾数位不一样，看图：

img

import torch
torch.finfo(torch.bfloat16)
# 结果
finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)
相比于 FP16，表示的精度降低了，但是表示的范围增加了，可以防止在训练过程中的溢出。

FP32
也叫做 float32，使用了32位二进制来表示浮点数。

img

一般来说 fp16的收敛速度要快一些，bf16用了更多的位来表示指数部分，留下了较少的位数用于表示有效数字，表示范围变大了，但更多的指数部分增加了计算和存储的开销

KV Cache 的占用字节数分析
image-20240826115042939

数据清洗和处理有哪些手段
使用传统的脚本设置正则化过滤，筛选有用数据，清除脏数据

	数据清洗的方法包括选择高质量数据源，规范化文本、去除HTML和标记、过滤停用词和噪声词，处理拼写错误、清除敏感信息，检测和处理异常值，评估数据质量，并记录清洗过程。

这些步骤有助于提高模型的准确性、稳定性和安全性，确保生成的输出更加可靠和高质量。

借助AI模型进行清洗。这涉及使用机器学习技术来培训智能体，使其能够自动识别和清洗数据，以优化人工与机器在数据清洗中的工作分配。
贝叶斯分类算法也被用于数据清洗，这是一种利用概率统计知识进行分类的算法。
还有一些尝试使用文本识别算法和识别技术等AI能力进行数据清洗。例如，决策树和随机森林算法具备根据特征辨别不良数据的能力。经典的ChatGPT训练数据通过OpenAI专门训练了一个过滤模型，用于识别有害内容，以确保对模型的输入和输出数据进行有效管控，以遵守使用政策。

人工审查

基座大模型结构解析
ChatGLM 升级之路
ChatGLM
使用了 Prefix-LM

ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型。ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。

ChatGLM2
有如下新特性

使用了更大的数据集

基于 FlashAttention 技术扩充了基座模型的上下文长度（2k -> 32k）

基于 Multi-Query Attention 技术，有更高效的推理速度和更低的显存占用

升级过程：

模型结构 从Prefix-LM回归纯粹的Decoder-Only结构

	为模型的训练效率带来了极大的提升。

序列长度

	预训练在32k数据集进行，微调在8k数据集进行

算子优化

	Flash Attention  Multi-Query Attention (MQA)

FlashAttention

计算复杂度和序列长度的平方N^2成正比，可以看一个小例子，比如两个相乘的矩阵大小分别为(N \times d) 和(d \times N)，矩阵乘法的一种计算方式是使用第一个矩阵的每一行与第二个矩阵的每一列做点乘。因为我们需要拿第一个矩阵的每一行去与第二个矩阵的每一列做点乘，所以总共就需要 N^2 次点乘。而每次点乘又需要 d 次乘法，所以总复杂度就为 O(N2d)

Self-Attention 的计算复杂度

image-20240826161155706

MLP 层的计算复杂度

image-20240826161425651

Logits 的计算量

image-20240826161711502

标准注意力Standard Attention的两个问题：显存占用多、HBM读写次数多

尽管已经有许多近似注意力的方法尝试减少attention的计算和内存要求。例如，稀疏近似和低秩近似的方法，将计算复杂度降低到了序列长度的线性或亚线性
但这些近似注意力方法方法并没有得到广泛应用。因为这些方法过于关注FLOPS(浮点数计算次数)的减少，而忽略了IO读写的内存访问开销，导致这并没有效减少运行时间(wall-clock time)
总之，在现代GPU中，计算速度已经远超过了显存访问速度，transformer中的大部分计算操作的瓶颈是显存访问。对于显存受限的操作，IO感知是非常重要的，因为显存读写占用了大部分的运行时间

总之，transformer的核心组件self-attention块的计算复杂度和空间复杂度是序列长度 的二次方
且对于self-attention块，除了两个大矩阵乘法是计算受限的，其他都是内存受限的逐点运算( 例如mask操作、softmax操作、dropout操作，这些逐点操作的性能是受限于内存带宽的，会减慢运行时间)

Memory-efficient Attention：把显存复杂度从平方降低到线性，但HBM访问次数仍是平方

在注意力计算过程中，节省显存的主要挑战是softmax与K,V的列是耦合的，其方法是单独计算softmax的归一化因子，来实现解耦。被称作 lazy softmax

Flash Attention：通过kernel融合降低HBM读写次数，避免频繁地从HBM中读写数据

对于性能受限于内存带宽的操作，进行加速的常用方式就是kernel融合，该操作的典型方式分为三步：

每个kernel将输入数据从低速的HBM中加载到高速的SRAM中

在SRAM中，进行计算

计算完毕后，将计算结果从SRAM中写入到HBM中

但是SRAM的内存大小有限，不可能一次性计算完整的注意力，因此必须进行分块计算，使得分块计算需要的内存不超过SRAM的大小。矩阵乘法的分块计算比较容易实现，难点在于Softmax的计算。因为计算归一化因子时，需要获得到完整的输入数据（Safe Softmax）。

为什么计算归一化因子时需要完整的输入数据呢，因为在深度学习中，为了避免发生的数值溢出的问题，计算时同窗会减去最大值，成为“Safe Softmax”

那到底怎么解决分块计算的难点——softmax的分块计算呢？考虑到softmax与 K 的列是耦合的，故可以通过引入了两个额外的统计量 m(x),l(x) 来进行解耦(*前者类似最大分数，后者类似exp分数总和*)，实现了分块计算

image-20240827110224452

LLama 升级之路
LLama2 
在模型结构上，主要升级两点：

训练数据Token数量从 1.4T -> 2T

序列长度从2k -> 4k

在预训练过程中，更强大的数据清洗，更新数据组合，增加40%的总训练tokens，加倍上下文长度，以及使用分组查询注意力（GQA）来提高更大模型的推理可扩展性。

在 SFT 过程中，LLAMA2强调数据质量的重要性，通过2W的高质量指令数据，激发模型的指令遵循能力。

在RLHF过程中，LLAMA2做了较多工作，对RLHF过程作出了进一步的解释。自建了100W的Reward数据集，训练了两个独立的Reword Model（Helpfulness RM / Safety RM）。

迭代训练

LLAMA2采用了两种强化学习算法:近端策略优化和拒绝采样算法。

这两种强化学习算法主要区别在于：

• 广度：在拒绝采样中，模型为给定的提示探索K个样本，而在PPO中，只有一个生成过程。

• 深度：在PPO中，训练过程中第t步的样本是经过t-1步梯度更新后的模型策略的函数。在拒绝采样微调中，在模型的初始策略下采样所有输出以收集新数据集，然后类似于SFT进行微调。然而，由于采用了迭代模型更新，这两种算法之间的本质区别并不明显。

LLAMA2直到RLHF (V4)，仅使用拒绝采样微调。之后将这两种方法结合起来，先对拒绝采样检查点应用PPO，然后再对采样进行拒绝采样。LLAMA2只使用最大的70B Llama 2-Chat模型进行拒绝采样。其他较小的模型则在更大模型的拒绝采样数据上进行微调，从而将大模型的能力转移到较小的模型中。

百川升级之路
baichuan-7b
百川模型结构与LLAMA相近，作了如下的优化：

分词器

参考学术界方案使用 SentencePiece 中的 Byte-Pair Encoding (BPE) 作为分词算法，并且进行了以下的优化：

使用 2000 万条以中英为主的多语言语料训练分词模型，显著提升对于中文的压缩率。

对数字的每一位单独分开，避免出现数字不一致的问题，对于提升数学能力有重要帮助

对于罕见字词（如特殊符号等），支持 UTF-8 characters 的 byte 编码，因此做到未知字词的全覆盖。

算子优化

采用更高效的算子:Flash-Attention，同ChatGLM2

baichuan-13B
参数量:baichuan13B较baichuan7B 首先在参数量上翻了一倍，更大的参数量意味着知识的容量更大，通过更多的训练数据(1.2T->1.4T)，基座模型的常识能力得以提升；

位置编码:从RoPE改成ALiBi，在一定程度的可以进行长度外推(TIPS:RoPE可以进行更长范围的外推)；

baichuan2
Positional Embeddings：

对于Baichuan 2-7B模型，采用了Rotary Positional Embedding (RoPE)。

对于Baichuan 2-13B模型，采用了ALiBi作为位置编码技术。

激活函数和归一化：

使用了SwiGLU激活函数，这是GLU的一个变体，经过改进的版本。

在注意力层中采用了内存高效的注意力机制。

Tokenizer：

对词汇表的大小进行了调整，将其从Baichuan 1的64,000扩展到125,696，以在计算效率和模型性能之间取得平衡。

NormHead：

Baichuan 2使用了一种称为NormHead的方法来稳定训练并提高模型性能。NormHead主要用于对输出嵌入进行归一化处理，有助于稳定训练动态，并降低了L2距离在计算logits时的影响。

最大z损失（Max-z loss）：

引入了最大z损失，用于规范模型输出的logit值，从而提高训练的稳定性并使推断更加鲁棒。

Transformer 参数量计算
image-20240827172904276

总的来说，每个transformer层的参数量为 12h^2+13h 。

除此之外还有词嵌入矩阵的参数量，V*h

什么是对比学习（Contrastive Learning）
对比学习是一种特殊的无监督学习方法。
旨在通过拉近相关样本的距离并且推远不相关样本的距离，来学习数据表示。应用对比学习可以训练出到非常具有泛化性Generalization的预训练模型或特征表述。

概念
Contrastive Learning (CL)与VAE、GAN 类似，都是可以作为Unsupervised Learning无监督学习方法的学习方式。但用在无标签数据时，通常我们会将对比学习归类为自监督学习Self-Supervised Learning，而不是无监督学习。

从人脸识别开始
例如 FaceNet，传统的人脸识别模型只要求一个人的不同照片要保持高相似度，FaceNet 创造了 Triplet Loss ，要求不同的人的不同照片也要报纸低相似度

而在對比學習與 FaceNet 最大的不同，在於對比學習的類內數據 (in-class data)不是收集來的，而是來自於數據擴增 Data Augmentation。

数据扩增
数据扩增可以将一笔数据扩增成N 笔数据，同时不影响这笔数据的意义。

通常实作时，我们会用预先设置好的机率值来触发某一种数据扩增方法。例如SimCLR 的GitHub 里面，当随机数< 机率p的时候就会触发数据扩增方法。数据扩增可以让我们增加数据的变化性，免费得到几倍以上的数据量，对最终模型的效果也有很重要的帮助。

从数据扩增到对比学习
Contrastive Learning 旨在学习一种高泛化性的通用特征提取方式，所提取的特征对于相似的样本有较高的相似度，而相异的样本有更低的相似度。

对比学习的五大要素
样本生成 ：从原始数据中生成成对样本，通常包括正样本对（相似的样本，例如同一对象的不同视角、不同时间的图像等）和负样本对（不相似的样本）。

特征提取 ：使用神经网络提取样本的特征表示。

相似性度量 ：计算正样本对和负样本对之间的相似性，常用的方法包括余弦相似度或欧几里得距离。

损失函数 ：使用对比损失（如对比损失函数、Triplet Loss等）来优化模型，使得正样本对的相似性尽量高，而负样本对的相似性尽量低。

训练与优化 ：通过反向传播和优化算法更新模型参数，使得模型能够更好地学习到样本的特征表示。

CLIP 与对比学习
CLIP（Contrastive Language–Image Pretraining）是OpenAI开发的一种模型，它通过对比学习来连接自然语言和图像，能够执行多种视觉和语言任务。

*对比学习*（Contrastive Learning）：

CLIP使用对比学习来训练模型，将配对的图像和文本表示拉近，而将不匹配的图像和文本表示推远。

通过这种方法，模型能够学习到图像和文本之间的相似性。

*多模态模型*（Multimodal Model）：

CLIP同时处理图像和文本数据，具备多模态的理解能力。

它由两个独立的编码器组成，一个用于图像（通常是基于ResNet或Vision Transformer的架构），另一个用于文本（通常是基于Transformer的架构）。

*大规模预训练*（Large-Scale Pretraining）：

CLIP在大规模的图像-文本对数据集上进行预训练，涵盖广泛的图像和文本内容。

这种预训练使得CLIP具备强大的泛化能力，可以适应多种下游任务。

在对比学习中，通常会使用两个网络（如一个目标网络和一个在线网络）来学习表示。在线网络的目标是预测目标网络在相同输入的不同增强版本上的表示。

List 和 Array 有什么样的区别，分别对应什么关键字
在计算机科学中，List（列表）和Array（数组）虽然都是用于存储数据的集合，但它们在功能、性能和用途上存在一些区别：

类型限定 ：

Array：通常数组中的元素类型是相同的。例如，一个整数数组只能包含整数。

List：列表则更加灵活，可以包含不同类型的元素，如整数、字符串、对象等。

内存分配 ：

Array：数组在内存中通常是连续分配的，这意味着访问数组元素的时间复杂度是O(1)。

List：列表在内存中可能是分散存储的，因此在某些语言中访问列表元素可能需要更多的时间。

大小可变性 ：

Array：数组的大小通常是固定的，一旦创建，其大小不能改变。

List：列表通常是可以动态扩展和缩减的。

性能 ：

Array：由于数组的内存连续性，它在处理大量数据时性能通常更优。

List：列表由于提供了更多的灵活性，可能在性能上略逊一筹。

对应到 Python 中：

Array：在 Python 中，如果你想要一个类似于其他语言中的数组，可以使用 array 模块，它提供了 array 类，用于创建固定类型的数组。例如，array('i', [1, 2, 3]) 创建了一个整数数组。对于数值计算，还常常使用 numpy 库中的 ndarray，它提供了更强大的数组功能。

List：在 Python 中，列表是通过方括号 [] 创建的，可以包含不同类型的元素，并且大小是可变的。例如，[1, 'a', 3.4] 是一个包含整数、字符串和浮点数的列表。

大模型推理加速方案
为什么推理速度慢
算法原因

生成过程必须在每个周期处理越来越多的 token

注意力机制需要大量的计算量

硬件原因

移动模型权重需要很多的时间

如何加速推理
可以将相关工作划分为三个优化层次：数据层、模型层、系统层

数据层优化技术：指通过优化输入提示词或规划模型输出内容优化推理效率。这类优化技术通常不需要修改模型本身，因此避免了大量的模型训练或微调开销

模型层优化技术：指通过设计高效的模型结构或模型压缩技术优化推理效率。这类技术通常需要对模型进行预训练或微调来恢复任务精度，同时通常对输出结果是有损的

系统层优化技术：指通过优化推理引擎或服务系统优化推理效率。这类技术通常不需要额外的模型训练开销，同时可以保证对输出结果是无损的。

数据层优化技术
可以分为两大类：输入压缩和输出规划

输入压缩：

提示词剪枝

提示词总结

基于软提示词的压缩：其中，软提示词指连续的、可学习的词块序列，可以通过训练的方式学习得到。

检索增强生成：通过检索和输入相关的辅助内容，并只将这些相关的内容加入到输入提示词中，来降低原本的输入长度。

输出规划：

“思维骨架”（Skeleton-of-Thought，以下简称SoT：SoT 技术的核心思想是让大语言模型自行规划输出的并行结构，并基于该结构进行并行解码，提升硬件利用率，减少端到端生成延时。

SoT 将大语言模型的生成分为两个阶段：在提纲阶段，SoT 通过设计的提示词让大语言模型输出答案的大纲；在分点扩展阶段，SoT 让大语言模型基于大纲中的每一个分点并行做扩展，最后将所有分点扩展的答案合并起来。

模型层优化技术
模型层优化技术可以划分为两大类：高效结构设计（Efficient Structure Design）和模型压缩（Model Compression）。

高效结构设计：

高效 FFN 设计

主流方法是混合专家（MoE）技术，其核心是为不同的输入词块分配不同数量的FFN（称为专家），减少推理过程中被激活的 FFN 权重数量。基于 MoE 的 Transformer 模型除了包含若干个专家 FFN 外，还包含一个负责分配专家的路由（Router）模型。

高效注意力算子设计：

多询问注意力技术：在不同的注意力头之间共享部分Key和Value来降低访存开销和内存占用；

低复杂度注意力技术：主要包括基于核函数（Kernel-based）的注意力算子和低秩注意力（Low-Rank）算子。

Transformer 替代架构：

最新的研究工作逐渐关注于设计新的模型架构来取代 Transformer 架构，这些新模型架构大多具有线性或近似线性的复杂度，在处理文本长度较长时有显著的性能优势。本综述将这类工作总结为两大类：

状态空间模型

非状态空间模型

模型压缩技术：

模型量化：

模型量化是一类应用广泛的模型压缩技术，其通过将模型的权重从高比特数转换成低比特数来降低模型的计算和存储开销。值得注意的是，大模型在推理的不同阶段有不一样的效率瓶颈，因此需要适配不同的量化方法。

在预填充阶段，推理效率主要受限于较高的计算量，因此通常采用权重激活量化（Weight-Activation Quantization）的方式；在解码阶段，推理效率主要受限于较高的访存量，因此通常采用仅权重量化（Weight-only Quantization）的方式。从量化流程来看，模型量化可以划分为：

训练后量化（Post-Training Quantization，PTQ）：指对预训练后的模型直接做量化，不需要重新训练量化后的模型。这类方法对量化后模型的精度损失较大，但因其不需要额外的模型训练开销，因此在大语言模型领域应用最为广泛。

训练感知量化（Quantization-Aware Training，QAT）：指在模型的训练中加入模型量化过程，并通过训练减小量化带来的精度损失。相比于训练后量化，训练感知量化方法通常具有更高的精度，但其需要大量的数据和计算量来做模型训练。因此，目前该子领域的研究主要关注于如何在数据层面和计算量层面降低模型训练带来的开销。

模型稀疏：

模型稀疏分为权重稀疏（权重剪枝）和注意力稀疏

权重剪枝（Weight Pruning）：指通过将模型中不重要的权重和对应结构移除，降低模型的计算和存储开销。权重剪枝可以分为非结构化剪枝（Unstructural Pruning）和结构化剪枝（Structural Pruning）两大类，两者区别主要在于剪枝粒度不同，如下图所示。其中，在非结构化剪枝领域，目前的研究工作主要关注于如何加速对大模型的剪枝过程，以及如何设计更有效的重要度分析指标和剪枝率分配策略。而在结构化剪枝领域，目前的研究工作大多关注于如何设计规整的剪枝结构来支持结构化的剪枝。

稀疏注意力（Sparse Attention）：指通过减少冗余的注意力计算，来降低预填充阶段的计算开销和解码阶段中 KV cache 带来存储和访存开销。该领域的研究工作主要关注于设计更有效的稀疏模式（Sparse Pattern），包括静态稀疏（下图中 (a) 和 (b) 所示）以及动态稀疏（下图中 (c) 和 (d) 所示）。无问芯穹于 2023 年 9 月发布的工作 SemSA[204]，通过对每个注意力头自动选择注意力掩膜和掩膜扩展方式，在平均稀疏度相同的情况下，大幅提升稀疏注意力大语言模型的有效上下文长达 3.9 倍。

结构优化：

结构优化技术指通过修改模型的架构或结构来达到更好的精度-效率之间的权衡。在该领域有两类代表性的技术

神经网络架构搜索（Neural Architecture Search）：指自动化地搜索出最优的模型架构。然而，这类方法目前只在中等规模的语言模型上得到应用，在大语言模型上还未获得实际的优化效果，原因是该技术通常需要在搜索过程中对采样到的架构进行训练评估，对大模型来说需要花费巨大的训练开销。

低秩分解（Low Rank Factorization）：指将一个大的权重矩阵近似分解成两个低质小矩阵的乘积，通过该技术，可以降低大语言模型权重的存储开销和访存开销。该领域的研究工作主要聚焦于设计分解方式，以及将该技术与其他模型压缩技术（例如量化、剪枝等）结合获得更高的效率优化效果。

知识蒸馏：使用一个大模型（教师模型）来训练一个小模型（学生模型），从而将大模型的知识传递给小模型。

白盒蒸馏：可以获得教师的模型架构和权重

黑盒蒸馏：基于 API 接口访问大模型

动态推理

系统层优化技术
大语言模型推理的系统层优化主要集中在优化模型的前向推理过程。在大语言模型的前向推理计算图中，注意力算子和线性算子占据了大部分运行时间。系统层优化技术包含对这两种算子的优化，同时还会考虑设计更高效的大语言模型解码方式。系统层优化技术分为高效的推理引擎（Inference Engine）和服务系统（Serving System）。

推理引擎：

图和算子优化

猜测解码：即通过并行解码来加速大模型解码过程的技术

服务系统：旨在提升系统处理异步请求的效率

内存管理

连续批处理

调度策略

分布式系统

什么是DeepSpeed
DeepSpeed是一个由微软开发的开源深度学习优化库，旨在提高大规模模型训练的效率和可扩展性。

DeepSpeed作为一个大模型训练加速库，位于模型训练框架和模型之间，用来提升训练、推理等。

核心架构
deepspeed主要包含三部分：

Apis。提供易用的api接口，训练模型、推理模型只需要简单调用几个接口即可。其中最重要的是initialize接口，用来初始化引擎，参数中配置训练参数及优化技术等。配置参数一般保存在config.json文件中。

runtime。运行时组件，是deepspeed管理、执行和性能优化的核心组件。如部署训练任务到分布式设备、数据分区、模型分区、系统优化、微调、故障检测、checkpoints保存和加载等。该组件使用python语言实现。

ops。用c++和cuda实现底层内核，优化计算和通信，例如ultrafast transformer kernels, fuse LAN kernels, customary deals等。

为什么使用这种架构？——架构设计的好处

可以在训练框架上进行两部分(训练和推理分开)优化；

与紧密耦合的结构比，该结构可以更好的利用整个生态，且与深度集成相比，更容易维护；

与基础设置无关，用户可以选择喜欢的平台，如Azure ML、Azure  VMs等

核心技术
ZeRO 零冗余优化器
零冗余优化器（Zero Redundancy Optimizer，缩写为Zero）是一种用于大规模分布式深度学习的新型内存优化技术。

ZeRO可以克服数据并行和模型并行的局限性，同时实现两者的优点。通过在数据并行进程之间划分模型状态参数、梯度和优化器状态来消除数据并行进程中的内存冗余，而不是复制它们。在训练期间使用动态通信调度来在分布式设备之间共享必要的状态，以保持数据并行的计算粒度和通信量。

ZeRO有三个主要的优化阶段（如下图所示），它们对应于优化器状态、梯度和参数的划分。

ZeRO-1：

主要通过分散优化器状态（如动量和梯度）来减少每个 GPU 的内存占用。

每个 GPU 只存储其所需的参数和优化器状态，其他的由其他 GPU 存储。

适用于模型参数较小的情况，可以显著减少内存开销。

ZeRO-2：

在 ZeRO-1 的基础上，进一步分散了模型参数。

每个 GPU 仅存储模型参数的一个子集，同时共享优化器状态和梯度。

通过这种方式，ZeRO-2 允许训练更大的模型，因为它能够将模型参数的内存占用分散到多个 GPU 上。

ZeRO-3：

是 ZeRO 的最先进版本，进一步增强了内存优化。

它不仅分散优化器状态和模型参数，还支持全量分散的梯度。

ZeRO-3 允许几乎不受限制地训练超大规模模型，因为它可以将所有模型参数、优化器状态和梯度在多个 GPU 之间进行分散，从而最大限度地减少每个 GPU 的内存占用

千问技术报告阅读
img

介绍
千问模型谱系：

基础语言模型，即QWEN，经过广泛的训练，使用了多达3万亿个各种文本和代码的标记，涵盖了广泛的领域。这些模型在多种下游任务中始终表现出优异的性能，即使与它们更大的同类模型相比也是如此。

QWEN-CHAT模型经过精心微调，使用了与任务执行、聊天、工具使用、代理、安全等相关的精心策划的数据集。基准评估表明，有监督微调模型可以实现更优异的性能。此外，我们训练了奖励模型来模仿人类偏好，并将其应用于RLHF，用于生成人类偏好的聊天模型响应。通过对具有挑战性的测试进行人类评估，我们发现使用RLHF训练的QWEN-CHAT模型具有很高的竞争力，但在我们的基准测试中仍然落后于GPT-4。

此外，我们提出了名为CODE-QWEN的专门模型，其中包括CODE-QWEN-7B和CODE-QWEN-14B，以及它们的聊天模型CODE-QWEN-14B-CHAT和CODE-QWEN-7B-CHAT。具体来说，CODE-QWEN已经在大量代码数据集上进行了预训练，并进一步微调以处理与代码生成、调试和解释相关的对话。在基准数据集上进行的实验结果，如HumanEval（Chen等，2021年）、MBPP（Austin等，2021年）和HumanEvalPack（Muennighoff等，2023年），表明了CODE-QWEN在代码理解和生成方面的高水平熟练度。

本研究还介绍了专门设计用于解决数学问题的MATH-QWEN-CHAT。我们的结果显示，无论是MATH-QWEN-7B-CHAT还是MATH-QWEN-14B-CHAT都优于相同规模的开源模型，并且正在接近与GPT-3.5在数学相关基准数据集（如GSM8K（Cobbe等，2021年）和MATH（Hendrycks等，2021年））上的性能。

此外，我们开源了QWEN-VL和QWEN-VL-CHAT，它们具有理解视觉和语言指令的多功能能力。这些模型在各种评估基准上优于当前的开源视觉-语言模型，并支持中文和英文的文本识别和视觉定位。此外，这些模型还支持多图像对话和叙述。更多细节可参见Bai等（2023年）。

预训练
数据
数据去重

精确匹配

MinHash LSH

过滤低质量数据

基于规则和机器学习的方法

数据采样

多任务指令

TOKENIZATION
高压缩效率

BPE tokenizer tiktoken

中文字符添加，数字拆分

架构
QWen 基于 Llama 进行了修改。

嵌入和输出投影 基于初步的实验结果，我们选择了非绑定的嵌入方法，而不是将输入嵌入和输出投影的权重捆绑在一起。这个决定是为了在内存成本的代价下实现更好的性能。

位置编码 我们选择RoPE（Rotary Positional Embedding）（Su等，2021年）作为将位置信息融入模型的首选方法。RoPE已被广泛采用，并在当代大型语言模型中取得了成功，特别是PaLM（Chowdhery等，2022年；Anil等，2023年）和LLaMA（Touvron等，2023a;b）。特别是，我们选择使用FP32精度来表示逆频率矩阵，而不是BF16或FP16，以优先考虑模型性能并实现更高的精度。

偏置对于大多数层，我们移除了偏置，遵循Chowdhery等人（2022年）的方法，但我们在注意力的QKV层中添加了偏置，以增强模型的外推能力（Su，2023b）。

Pre-Normalization 和 RMSNorm 在现代Transformer模型中，pre-normalization 是最常用的方法，已经证明与post-normalization 相比可以提高训练稳定性。最近的研究提出了更好的训练稳定性的替代方法，我们计划在模型的未来版本中探索这些方法。此外，我们将传统的层标准化技术（Ba等人，2016年）替换为RMSNorm（Jiang等人，2023年）。这一改变在提高效率的同时也实现了等效的性能。

image-20240912114655579

激活函数 我们选择SwiGLU（Shazeer，2020年）作为我们的激活函数，它是Swish（Ramachandran等人，2017年）和门控线性单元（Dauphin等人，2017年）的组合。我们的初步实验表明，基于GLU的激活函数通常优于其他基线选项，如GeLU（Hendrycks＆Gimpel，2016年）。与以前的研究中的常见做法一样，我们将前馈网络（feed-forward network，FFN）的维度从隐藏大小的4倍减少到隐藏大小的三分之八。

SwiGLU(x)=Swish(x)⋅σ(W2x+b2)。 SwiGLU 结合了 Swish 的平滑性，能够在训练过程中提供更好的梯度流动，减少死神经元的问题。通过引入门控机制，SwiGLU 能够选择性地通过某些信息，这样可以增强模型的表达能力。

训练
自回归语言建模的标准方法，训练模型根据前面的标记提供的上下文来预测下一个标记。我们训练具有2048个上下文长度的模型。

为了提高计算效率和减少内存使用，我们在注意力模块中采用了Flash Attention

模型都使用BFloat16混合精度进行训练稳定性

对齐
使用对其技术如监督微调和从人类反馈中强化学习可以显著提高模型参与自然对话的能力

尽管SFT已被证明是有效的，但我们承认它的泛化和创造力能力可能受到限制，并且容易出现过拟合问题。为解决这个问题，我们实施了从人类反馈中进行强化学习（RLHF），以进一步使SFT模型与人类偏好保持一致，采用了Ouyang等人（2022）和Christiano等人（2017）的方法。这个过程涉及训练一个奖励模型，并使用近端策略优化（PPO）（Schulman等人，2017）进行策略训练。

Qwen2 技术报告阅读
引言
我们的发布包括四个稠密模型，参数计数分别为0.5b、1.5b、7b和72b，以及一个具有57b参数的专家混合（MoE）模型，每个标记激活14亿参数。

所有模型都是在超过7 trillion token（7万亿）的高质量、大规模数据集上预训练的，涵盖了广泛的领域和语言。与以前的Qwen版本相比，Qwen2包含了更广泛的语言数据，增强了代码和数学内容的数量和质量。这种丰富被假设为提高LLMs的推理能力。关于后训练，所有模型都经过了有监督的微调和直接偏好优化（DPO）

从 PPO 升级到 DPO 了

PPO 工作原理

目标函数：PPO旨在通过最大化特定的目标函数来改进策略。这个目标函数通常包括一个期望回报的项，以及可能的正则化项（如熵）来鼓励探索。

概率比率剪切：PPO使用了一种称为概率比率剪切的技术，这涉及到计算新策略和旧策略对动作概率的比率。如果这个比率偏离1太远，PPO会通过剪切这个比率来限制更新的幅度，从而避免过大的策略变动。

目标函数的优化：PPO对目标函数进行优化，通常使用随机梯度上升方法。这个过程涉及到在策略网络参数上应用梯度更新，以增加高回报动作的概率，同时减少低回报动作的概率。

多次迭代更新：PPO算法通常在一次策略更新中使用多个迭代，这意味着它会重复利用同一批数据多次，以进行有效的学习。

PPO 实现步骤

收集数据：首先，使用当前策略在环境中执行多个动作，收集状态、动作和回报的数据。

计算优势函数：然后，计算每个时间步的优势函数，这通常涉及到对回报的估计和基线（比如状态价值函数）的使用。

优化策略：接着，通过优化目标函数来更新策略参数。这个过程包括计算目标函数的梯度，并使用梯度上升来更新参数。

重复迭代：重复上述过程多次，直到策略收敛或达到预定的迭代次数。

DPO 工作原理

DPO是一种相对较新的方法，它直接优化用户或专家的偏好，而非传统的累积奖励。在DPO中，通过对比不同的决策序列或策略，并根据用户或专家的偏好来优化模型，使得最终的策略能够更好地符合预期的行为。DPO通常用于那些难以明确定义奖励函数的场景，或者在用户偏好需要直接编码到决策过程中的应用中。
DPO的实现需要构建一个偏好模型，该模型能够从用户或专家的反馈中学习。在实际应用中，可能需要设计一种机制来收集用户的偏好数据，例如通过对比查询或者排名反馈。然后使用这些数据来训练一个或多个模型，这些模型能够预测给定决策序列的偏好得分，并据此来优化策略。

只需要加载2个模型，其中一个推理，另外一个训练，直接在偏好数据上进行训练。

Tokenizer & Model
沿用Qwen（Bai等人，2023a）的做法，我们采用了基于字节级字节对编码的相同Tokenizer。值得注意的是，该分词器展现出高编码效率，这得益于其相对于替代方案更好的压缩率，这有助于Qwen2的多语言能力。

Qwen2系列基本上是基于Transformer架构的大型语言模型，具有自注意力和因果掩码（Vaswani等人，2017）。具体来说，该系列包括4个规模的稠密语言模型和一个专家混合（MoE）模型。我们在深入MoE模型的独特属性之前，先介绍稠密模型的细节。

Qwen2密集模型的架构包括多个Transformer层，每层都配备了因果注意力机制和前馈神经网络（FFN）。与Qwen的主要区别如下：

分组查询注意力：我们采用了分组查询注意力（Grouped Query Attention，GQA，Ainslie等人，2023）而不是传统的多头注意力（multi-head attention，MHA）。GQA在推理期间优化了KV缓存的使用，显著提高了吞吐量。不同模型大小的详细KV头配置在第2.2.3节报告。

双块注意力与YARN：为了扩大Qwen2的上下文窗口，我们实现了双块注意力（Dual Chunk Attention，DCA，An等人，2024），它将长序列分割成可管理的长度块。如果输入可以在一个块中处理，DCA产生与原始注意力相同的结果。否则，DCA有助于在块内和跨块之间有效地捕获相对位置信息，从而提高长上下文性能。此外，我们还采用了YARN（Peng等人，2023）来重新调整注意力权重，以实现更好的长度外推。

我们还沿用了Qwen的使用，包括SwiGLU（Dauphin等人，2017）作为激活函数，旋转位置嵌入（RoPE，Su等人，2024）作为位置嵌入，QKV偏置（Su，2023）用于注意力，RMSNorm（Jiang等人，2023b）和预归一化用于训练稳定性。
